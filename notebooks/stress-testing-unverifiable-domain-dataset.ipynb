{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport os\nfrom abc import ABC, abstractmethod\nimport pandas as pd\nimport json\nfrom typing import List, Dict, Any\nfrom collections import Counter\nimport numpy as np\nimport os\nimport json\nimport random\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_from_disk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:58:03.542501Z","iopub.execute_input":"2026-01-06T08:58:03.543301Z","iopub.status.idle":"2026-01-06T08:58:10.006942Z","shell.execute_reply.started":"2026-01-06T08:58:03.543271Z","shell.execute_reply":"2026-01-06T08:58:10.006142Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nfrom datasets import load_dataset\n\n# Create directory for HF datasets\nos.makedirs('/kaggle/working/hf_datasets', exist_ok=True)\n\n# 1. Infinite Chats (Conversation/Unverifiable)\nprint(\"1/4 Downloading infinite-chats-eval...\")\nic = load_dataset(\"liweijiang/infinite-chats-eval\")\nic.save_to_disk('/kaggle/working/hf_datasets/infinite-chats-eval')\n\n# 2. GovReport (Summarization)\nprint(\"\\n2/4 Downloading govreport-summarization...\")\ngr = load_dataset(\"ccdv/govreport-summarization\")\ngr.save_to_disk('/kaggle/working/hf_datasets/govreport-summarization')\n\n# 3. CNN/DailyMail (Summarization) - replacing XSum\nprint(\"\\n3/4 Downloading cnn_dailymail...\")\ncnn = load_dataset(\"cnn_dailymail\", \"3.0.0\", trust_remote_code=True)\ncnn.save_to_disk('/kaggle/working/hf_datasets/cnn_dailymail')\n\n# 4. WritingPrompts (Creative Writing)\nprint(\"\\n4/4 Downloading writingprompts...\")\nwp = load_dataset(\"euclaise/writingprompts\")\nwp.save_to_disk('/kaggle/working/hf_datasets/writingprompts')\n\nprint(\"\\nâœ… All unverifiable domain datasets downloaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T09:02:35.180624Z","iopub.execute_input":"2026-01-06T09:02:35.180932Z","iopub.status.idle":"2026-01-06T09:02:52.178823Z","shell.execute_reply.started":"2026-01-06T09:02:35.180911Z","shell.execute_reply":"2026-01-06T09:02:52.178010Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"1/4 Downloading infinite-chats-eval...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979347687d9643289f0b2c7360537550"}},"metadata":{}},{"name":"stdout","text":"\n2/4 Downloading govreport-summarization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/17517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95eea608e3274db886968105eb2433d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/973 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8119ff635ac94f7aac9cb7a368f9bf59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/973 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52943c2997a84665977e745d1d4bf37c"}},"metadata":{}},{"name":"stdout","text":"\n4/4 Downloading writingprompts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/837 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9ff0470b7e4037be19db75fb844316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002-105e07cb0d1994(â€¦):   0%|          | 0.00/272M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8237dc3d8564294b322a4ff94052fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002-4fdb982c110564(â€¦):   0%|          | 0.00/272M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc2c37e293b4af09f7e8bef565f2677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-16503b0c26ed00c(â€¦):   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45f2ddd540445ffb5a2355168688cc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-137b93e1e(â€¦):   0%|          | 0.00/30.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df83383e4a44f60aae955c5a1b98141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/272600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c456949498654837994153094d234680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/15138 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e2693e3d0d47c786e916250314e347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/15620 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8dc078cc6949fc8a735c17d399e5c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/272600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d548dcfd32d42b1b6c7298223f24d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/15138 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d520e1171d9a4f03859f737ef73b7f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/15620 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d630b7daf91e4fe596a5a4609058e354"}},"metadata":{}},{"name":"stdout","text":"\nâœ… All unverifiable domain datasets downloaded!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==================== CONFIGURATION ====================\n\n# Output path\nOUTPUT_PATH = \"/kaggle/working/unverifiable_test_dataset.jsonl\"\n\n# Sample sizes per domain\nSAMPLES_PER_DOMAIN = 100\n\n# Domains\nDOMAINS = [\n    \"creative_writing\",\n    \"summarization\", \n    \"conversation\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T09:04:10.916918Z","iopub.execute_input":"2026-01-06T09:04:10.917437Z","iopub.status.idle":"2026-01-06T09:04:10.922468Z","shell.execute_reply.started":"2026-01-06T09:04:10.917409Z","shell.execute_reply":"2026-01-06T09:04:10.921454Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ==================== UTILITIES ====================\n\ndef to_json_serializable(obj: Any) -> Any:\n    \"\"\"Convert numpy/pandas objects to JSON-serializable Python types\"\"\"\n    if isinstance(obj, (np.ndarray, pd.Series)):\n        return obj.tolist()\n    elif isinstance(obj, (np.integer, np.floating)):\n        return obj.item()\n    elif isinstance(obj, dict):\n        return {k: to_json_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        return [to_json_serializable(item) for item in obj]\n    elif pd.isna(obj):\n        return None\n    else:\n        return obj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T09:04:13.417619Z","iopub.execute_input":"2026-01-06T09:04:13.418623Z","iopub.status.idle":"2026-01-06T09:04:13.425667Z","shell.execute_reply.started":"2026-01-06T09:04:13.418596Z","shell.execute_reply":"2026-01-06T09:04:13.424548Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==================== BASE ADAPTER ====================\n\nclass DatasetAdapter(ABC):\n    \"\"\"Base adapter for all datasets\"\"\"\n    \n    def __init__(self, domain: str, source_name: str):\n        self.domain = domain\n        self.source_name = source_name\n    \n    @abstractmethod\n    def load(self) -> pd.DataFrame:\n        \"\"\"Load raw data\"\"\"\n        pass\n    \n    @abstractmethod\n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform to unified schema\"\"\"\n        pass\n    \n    def validate(self, data: List[Dict]) -> List[Dict]:\n        \"\"\"Validate required fields and quality\"\"\"\n        validated = []\n        for item in data:\n            # Check required fields\n            if not all(k in item for k in ['domain', 'prompt']):\n                continue\n            \n            # Check prompt length (>5 words)\n            if len(item['prompt'].split()) < 5:\n                continue\n            \n            validated.append(item)\n        \n        return validated\n    \n    def process(self, max_samples: int = None) -> List[Dict]:\n        \"\"\"Full pipeline: load â†’ transform â†’ validate â†’ sample\"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"Processing {self.source_name}...\")\n        print(f\"{'='*50}\")\n        \n        try:\n            # Load\n            print(\"  [1/4] Loading raw data...\")\n            df = self.load()\n            print(f\"        Loaded {len(df)} rows\")\n            \n            # Transform\n            print(\"  [2/4] Transforming to unified schema...\")\n            transformed = self.transform(df)\n            print(f\"        Transformed {len(transformed)} samples\")\n            \n            # Validate\n            print(\"  [3/4] Validating...\")\n            validated = self.validate(transformed)\n            print(f\"        Validated {len(validated)} samples\")\n            \n            # Sample if needed\n            if max_samples and len(validated) > max_samples:\n                print(f\"  [4/4] Sampling {max_samples} from {len(validated)}...\")\n                validated = random.sample(validated, max_samples)\n            \n            return validated\n            \n        except Exception as e:\n            print(f\"  ERROR: {e}\")\n            import traceback\n            traceback.print_exc()\n            return []\n\n\n# ==================== CONCRETE ADAPTERS ====================\n\nclass InfiniteChatsAdapter(DatasetAdapter):\n    \"\"\"Adapter for liweijiang/infinite-chats-eval\n    \n    Format: Single column with conversation/chat data for unverifiable evaluation.\n    We use each sample as a conversational prompt.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__(domain=\"conversation\", source_name=\"Infinite-Chats-Eval\")\n    \n    def load(self) -> pd.DataFrame:\n        from datasets import load_dataset\n        dataset = load_dataset(\"liweijiang/infinite-chats-eval\", split=\"train\")\n        return dataset.to_pandas()\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        data = []\n        \n        # Get the first column name (single column dataset)\n        col_name = df.columns[0]\n        \n        for _, row in df.iterrows():\n            content = str(row[col_name]).strip()\n            \n            if len(content) > 20:  # Minimum length\n                # Use the content as a conversation prompt\n                data.append({\n                    \"domain\": self.domain,\n                    \"prompt\": content[:2000],  # Truncate if too long\n                    \"answer\": \"\",  # No ground truth for unverifiable\n                    \"metadata\": {\"source\": self.source_name}\n                })\n        \n        return data\n\n\nclass GovReportAdapter(DatasetAdapter):\n    \"\"\"Adapter for ccdv/govreport-summarization\n    \n    Format: Has 'report' (full text) and 'summary' columns.\n    We use the summary and ask to summarize it further (meta-summarization).\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__(domain=\"summarization\", source_name=\"GovReport\")\n    \n    def load(self) -> pd.DataFrame:\n        from datasets import load_dataset\n        dataset = load_dataset(\"ccdv/govreport-summarization\", split=\"test\")\n        return dataset.to_pandas()\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        data = []\n        \n        for _, row in df.iterrows():\n            summary = row.get('summary', '')\n            \n            if summary and len(summary) > 100:\n                # Truncate very long summaries\n                summary_text = str(summary)[:2000]\n                \n                prompt = f\"Summarize the following text in 2-3 sentences:\\n\\n{summary_text}\"\n                \n                data.append({\n                    \"domain\": self.domain,\n                    \"prompt\": prompt,\n                    \"answer\": \"\",  # No ground truth for meta-summary\n                    \"metadata\": {\"source\": self.source_name}\n                })\n        \n        return data\n\n\nclass CNNDailyMailAdapter(DatasetAdapter):\n    \"\"\"Adapter for cnn_dailymail (replacing XSum which has deprecated scripts)\n    \n    Format: Has 'article' and 'highlights' columns.\n    We use the article and ask for a summary.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__(domain=\"summarization\", source_name=\"CNN-DailyMail\")\n    \n    def load(self) -> pd.DataFrame:\n        from datasets import load_dataset\n        dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\", trust_remote_code=True)\n        return dataset.to_pandas()\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        data = []\n        \n        for _, row in df.iterrows():\n            article = row.get('article', '')\n            highlights = row.get('highlights', '')\n            \n            if article and len(article) > 100:\n                # Truncate long articles\n                article_text = str(article)[:1500]\n                \n                prompt = f\"Provide a brief summary of the following news article:\\n\\n{article_text}\"\n                \n                data.append({\n                    \"domain\": self.domain,\n                    \"prompt\": prompt,\n                    \"answer\": highlights,  # Original highlights as reference\n                    \"metadata\": {\"source\": self.source_name}\n                })\n        \n        return data\n\n\nclass WritingPromptsAdapter(DatasetAdapter):\n    \"\"\"Adapter for euclaise/writingprompts\n    \n    Format: Has prompts for creative story writing.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__(domain=\"creative_writing\", source_name=\"WritingPrompts\")\n    \n    def load(self) -> pd.DataFrame:\n        from datasets import load_dataset\n        dataset = load_dataset(\"euclaise/writingprompts\", split=\"train\")\n        return dataset.to_pandas()\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        data = []\n        \n        # Check column names\n        prompt_col = 'prompt' if 'prompt' in df.columns else df.columns[0]\n        story_col = 'story' if 'story' in df.columns else (df.columns[1] if len(df.columns) > 1 else None)\n        \n        for _, row in df.iterrows():\n            prompt = str(row[prompt_col]).strip()\n            \n            # Clean common prefixes from writing prompts\n            for prefix in ['[WP]', '[SP]', '[EU]', '[CW]', '[RF]', '[TT]']:\n                prompt = prompt.replace(prefix, '').strip()\n            \n            if len(prompt) > 20:\n                full_prompt = f\"Write a short story based on this prompt:\\n\\n{prompt}\"\n                \n                data.append({\n                    \"domain\": self.domain,\n                    \"prompt\": full_prompt,\n                    \"answer\": str(row[story_col])[:500] if story_col else \"\",  # Sample of story if available\n                    \"metadata\": {\"source\": self.source_name}\n                })\n        \n        return data\n\n\n# ==================== DATASET MIXER ====================\n\nclass UnverifiableDomainMixer:\n    \"\"\"Combines all adapters into unverifiable domain test dataset\"\"\"\n    \n    def __init__(self):\n        self.adapters = [\n            InfiniteChatsAdapter(),\n            GovReportAdapter(),\n            CNNDailyMailAdapter(),\n            WritingPromptsAdapter(),\n        ]\n    \n    def create_dataset(self, samples_per_domain: int = 100) -> List[Dict]:\n        \"\"\"Create balanced unverifiable domain dataset\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"CREATING UNVERIFIABLE DOMAINS DATASET\")\n        print(\"=\"*60)\n        \n        all_data = []\n        \n        # Process each adapter\n        for adapter in self.adapters:\n            data = adapter.process(max_samples=samples_per_domain)\n            all_data.extend(data)\n        \n        # Shuffle\n        random.seed(42)\n        random.shuffle(all_data)\n        \n        return all_data\n    \n    def save_dataset(self, data: List[Dict], output_path: str):\n        \"\"\"Save dataset as JSONL\"\"\"\n        with open(output_path, 'w') as f:\n            for item in data:\n                serializable_item = to_json_serializable(item)\n                f.write(json.dumps(serializable_item) + '\\n')\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"SAVED UNVERIFIABLE DOMAINS DATASET\")\n        print(f\"{'='*60}\")\n        print(f\"Location: {output_path}\")\n        print(f\"Total samples: {len(data)}\")\n        \n        # Print domain distribution\n        domain_counts = Counter(item['domain'] for item in data)\n        print(f\"\\nðŸ“Š Domain Distribution:\")\n        for domain in sorted(domain_counts.keys()):\n            count = domain_counts[domain]\n            pct = (count / len(data)) * 100\n            print(f\"  {domain:20s}: {count:5d} ({pct:5.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T09:10:06.351978Z","iopub.execute_input":"2026-01-06T09:10:06.352710Z","iopub.status.idle":"2026-01-06T09:10:06.379838Z","shell.execute_reply.started":"2026-01-06T09:10:06.352683Z","shell.execute_reply":"2026-01-06T09:10:06.378843Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"UNVERIFIABLE DOMAINS DATASET CURATION\")\nprint(\"=\"*60)\n\n# Create mixer\nmixer = UnverifiableDomainMixer()\n\n# Create dataset\ndata = mixer.create_dataset(samples_per_domain=SAMPLES_PER_DOMAIN)\n\n# Save\nmixer.save_dataset(data, OUTPUT_PATH)\n\n# Save sample for inspection\nsample_path = OUTPUT_PATH.replace('.jsonl', '_sample.jsonl')\nwith open(sample_path, 'w') as f:\n    for item in data[:20]:\n        f.write(json.dumps(to_json_serializable(item)) + '\\n')\n\nprint(f\"\\nSaved 20 samples to {sample_path}\")\nprint(\"\\nâœ… UNVERIFIABLE DOMAINS DATASET CREATION COMPLETE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T09:10:06.964453Z","iopub.execute_input":"2026-01-06T09:10:06.964900Z","iopub.status.idle":"2026-01-06T09:10:24.927001Z","shell.execute_reply.started":"2026-01-06T09:10:06.964876Z","shell.execute_reply":"2026-01-06T09:10:24.926223Z"}},"outputs":[{"name":"stdout","text":"============================================================\nUNVERIFIABLE DOMAINS DATASET CURATION\n============================================================\n\n============================================================\nCREATING UNVERIFIABLE DOMAINS DATASET\n============================================================\n\n==================================================\nProcessing Infinite-Chats-Eval...\n==================================================\n  [1/4] Loading raw data...\n        Loaded 100 rows\n  [2/4] Transforming to unified schema...\n        Transformed 100 samples\n  [3/4] Validating...\n        Validated 100 samples\n\n==================================================\nProcessing GovReport...\n==================================================\n  [1/4] Loading raw data...\n","output_type":"stream"},{"name":"stderr","text":"`trust_remote_code` is not supported anymore.\nPlease check that the Hugging Face dataset 'cnn_dailymail' isn't based on a loading script and remove `trust_remote_code`.\nIf the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n","output_type":"stream"},{"name":"stdout","text":"        Loaded 973 rows\n  [2/4] Transforming to unified schema...\n        Transformed 973 samples\n  [3/4] Validating...\n        Validated 973 samples\n  [4/4] Sampling 100 from 973...\n\n==================================================\nProcessing CNN-DailyMail...\n==================================================\n  [1/4] Loading raw data...\n        Loaded 11490 rows\n  [2/4] Transforming to unified schema...\n        Transformed 11490 samples\n  [3/4] Validating...\n        Validated 11490 samples\n  [4/4] Sampling 100 from 11490...\n\n==================================================\nProcessing WritingPrompts...\n==================================================\n  [1/4] Loading raw data...\n        Loaded 272600 rows\n  [2/4] Transforming to unified schema...\n        Transformed 268807 samples\n  [3/4] Validating...\n        Validated 268807 samples\n  [4/4] Sampling 100 from 268807...\n\n============================================================\nSAVED UNVERIFIABLE DOMAINS DATASET\n============================================================\nLocation: /kaggle/working/unverifiable_test_dataset.jsonl\nTotal samples: 400\n\nðŸ“Š Domain Distribution:\n  conversation        :   100 ( 25.0%)\n  creative_writing    :   100 ( 25.0%)\n  summarization       :   200 ( 50.0%)\n\nSaved 20 samples to /kaggle/working/unverifiable_test_dataset_sample.jsonl\n\nâœ… UNVERIFIABLE DOMAINS DATASET CREATION COMPLETE!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}