{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566},{"sourceId":4549361,"sourceType":"datasetVersion","datasetId":2656057},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119},{"sourceId":7114700,"sourceType":"datasetVersion","datasetId":4102922},{"sourceId":14291659,"sourceType":"datasetVersion","datasetId":9122651}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preparion \n- Join multiple dataset resources into a single dataset.\n- New Dataset will be used to fine tune Gemma 3 1B model. ","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport os\nfrom abc import ABC, abstractmethod\nimport pandas as pd\nimport json\nfrom typing import List, Dict, Any\nfrom collections import Counter\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Download From HuggingFace","metadata":{}},{"cell_type":"code","source":"# Create directory for HF datasets\nos.makedirs('/kaggle/working/hf_datasets', exist_ok=True)\n\nprint(\"Downloading HuggingFace datasets...\")\n\n# 1. WritingPrompts\nprint(\"\\n1/4 WritingPrompts...\")\nwp = load_dataset(\"euclaise/writingprompts\")\nwp.save_to_disk('/kaggle/working/hf_datasets/writingprompts')\n\n# 2. StrategyQA\nprint(\"\\n2/4 StrategyQA...\")\ntry:\n    sqa = load_dataset(\"wics/strategy-qa\", trust_remote_code=True)\nexcept:\n    sqa = load_dataset(\"metaeval/strategy-qa\")\nsqa.save_to_disk('/kaggle/working/hf_datasets/strategy-qa')\n\n# 3. No Robots\nprint(\"\\n3/4 No Robots...\")\nnr = load_dataset(\"HuggingFaceH4/no_robots\")\nnr.save_to_disk('/kaggle/working/hf_datasets/no_robots')\n\n# 4. Dolly-15k\nprint(\"\\n4/4 Dolly-15k...\")\ndolly = load_dataset(\"databricks/databricks-dolly-15k\")\ndolly.save_to_disk('/kaggle/working/hf_datasets/dolly-15k')\n\nprint(\"\\nAll HuggingFace datasets downloaded!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SETUP","metadata":{}},{"cell_type":"code","source":"DATASET_CONFIG = {\n    \"username\": \"fissalalsharef\",\n    \"dataset_slug\": \"temporal-flux-calibration-v2\",\n    \n    # Kaggle datasets (accessible via /kaggle/input/)\n    \"gsm8k\": {\n        \"path\": \"/kaggle/input/grade-school-math-8k-q-a/main_train.csv\",\n        \"format\": \"csv\",\n        \"domain\": \"math\",\n    },\n    \"mbpp\": {\n        \"path\": \"/kaggle/input/mbppjsonl/mbpp.jsonl\",\n        \"format\": \"jsonl\",\n        \"domain\": \"coding\",\n    },\n    \"sciq\": {\n        \"paths\": {\n            \"train\": \"/kaggle/input/sciq-a-dataset-for-science-question-answering/train.csv\",\n            \"validation\": \"/kaggle/input/sciq-a-dataset-for-science-question-answering/validation.csv\",\n            \"test\": \"/kaggle/input/sciq-a-dataset-for-science-question-answering/test.csv\",\n        },\n        \"format\": \"csv\",\n        \"domain\": \"science\",\n    },\n    \"cnn_dailymail\": {\n        \"paths\": {\n            \"train\": \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\",\n            \"validation\": \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\",\n            \"test\": \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\",\n        },\n        \"format\": \"csv\",\n        \"domain\": \"summarization\",\n        \"max_samples\": 10000,  # Cap to avoid overloading\n    },\n    \n    # HuggingFace datasets (downloaded to /kaggle/working/hf_datasets/)\n    \"strategy_qa\": {\n        \"path\": \"/kaggle/working/hf_datasets/strategy-qa\",\n        \"format\": \"hf_dataset\",\n        \"domain\": \"logic\",\n    },\n    \"writing_prompts\": {\n        \"path\": \"/kaggle/working/hf_datasets/writingprompts\",\n        \"format\": \"hf_dataset\",\n        \"domain\": \"creative_writing\",\n    },\n    \"dolly\": {\n        \"path\": \"/kaggle/working/hf_datasets/dolly-15k\",\n        \"format\": \"hf_dataset\",\n        \"domain\": \"creative_ideation\",\n        \"filter_category\": \"brainstorming\",\n    },\n    \"no_robots\": {\n        \"path\": \"/kaggle/working/hf_datasets/no_robots\",\n        \"format\": \"hf_dataset\",\n        \"domain\": \"creative_ideation\",\n        \"filter_category\": \"Brainstorm\",\n    },\n}\n\n# Unified schema\nUNIFIED_SCHEMA = {\n    \"domain\": \"Domain category\",\n    \"prompt\": \"The task/question\",\n    \"answer\": \"Ground truth response\",\n    \"metadata\": \"Optional source-specific data\"\n}\n\n# Domain list\nDOMAINS = [\n    \"math\",\n    \"coding\", \n    \"science\",\n    \"summarization\",\n    \"logic\",\n    \"creative_writing\",\n    \"creative_ideation\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_json_serializable(obj: Any) -> Any:\n    \"\"\"Convert numpy/pandas objects to JSON-serializable Python types\"\"\"\n    if isinstance(obj, (np.ndarray, pd.Series)):\n        return obj.tolist()\n    elif isinstance(obj, (np.integer, np.floating)):\n        return obj.item()\n    elif isinstance(obj, dict):\n        return {k: to_json_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        return [to_json_serializable(item) for item in obj]\n    elif pd.isna(obj):\n        return None\n    else:\n        return obj","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Base Class Adapter","metadata":{}},{"cell_type":"code","source":"class DatasetAdapter(ABC):\n    \"\"\"Base adapter for all datasets\"\"\"\n    \n    def __init__(self, domain: str, source_name: str):\n        self.domain = domain\n        self.source_name = source_name\n    \n    @abstractmethod\n    def load(self) -> pd.DataFrame:\n        \"\"\"Load ALL raw data (merge all splits if needed)\"\"\"\n        pass\n    \n    @abstractmethod\n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform to unified schema\"\"\"\n        pass\n    \n    def validate(self, data: List[Dict]) -> List[Dict]:\n        \"\"\"Validate required fields and quality\"\"\"\n        validated = []\n        for item in data:\n            # Check required fields\n            if not all(k in item for k in ['domain', 'prompt', 'answer']):\n                continue\n            \n            # Check prompt length (>5 words)\n            if len(item['prompt'].split()) < 5:\n                continue\n            \n            # Check answer not empty\n            if len(item['answer'].strip()) == 0:\n                continue\n            \n            validated.append(item)\n        \n        return validated\n    \n    def process(self) -> List[Dict]:\n        \"\"\"Full pipeline: load â†’ transform â†’ validate\"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"Processing {self.source_name}...\")\n        print(f\"{'='*50}\")\n        \n        # Load\n        print(\"  [1/3] Loading raw data...\")\n        df = self.load()\n        print(f\"        Loaded {len(df)} rows\")\n        \n        # Transform\n        print(\"  [2/3] Transforming to unified schema...\")\n        transformed = self.transform(df)\n        print(f\"        Transformed {len(transformed)} samples\")\n        \n        # Validate\n        print(\"  [3/3] Validating...\")\n        validated = self.validate(transformed)\n        print(f\"        Validated {len(validated)} samples\")\n        \n        return validated","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GSM8KAdapter(DatasetAdapter):\n    \"\"\"Adapter for GSM8K math dataset from Kaggle\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"gsm8k\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"GSM8K\")\n        self.path = config[\"path\"]\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load GSM8K dataset (single file, no splits)\"\"\"\n        return pd.read_csv(self.path)\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform GSM8K to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": row['question'],\n                \"answer\": str(row['answer']),\n                \"metadata\": {\n                    \"source\": self.source_name,\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: MBPP (Coding)\n# ============================================================\n\nclass MBPPAdapter(DatasetAdapter):\n    \"\"\"Adapter for MBPP coding dataset from Kaggle\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"mbpp\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"MBPP\")\n        self.path = config[\"path\"]\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load MBPP JSONL file\"\"\"\n        data = []\n        with open(self.path) as f:\n            for line in f:\n                data.append(json.loads(line))\n        return pd.DataFrame(data)\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform MBPP to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": row['text'],\n                \"answer\": row['code'],\n                \"metadata\": {\n                    \"source\": self.source_name,\n                    \"test_cases\": row.get('test_list', []),\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: SciQ (Science)\n# ============================================================\n\nclass SciQAdapter(DatasetAdapter):\n    \"\"\"Adapter for SciQ science dataset from Kaggle\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"sciq\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"SciQ\")\n        self.paths = config[\"paths\"]\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load and merge all SciQ splits\"\"\"\n        train = pd.read_csv(self.paths[\"train\"])\n        val = pd.read_csv(self.paths[\"validation\"])\n        test = pd.read_csv(self.paths[\"test\"])\n        return pd.concat([train, val, test], ignore_index=True)\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform SciQ to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": row['question'],\n                \"answer\": row['correct_answer'],\n                \"metadata\": {\n                    \"source\": self.source_name,\n                    \"support\": row.get('support', ''),\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: CNN/DailyMail (Summarization)\n# ============================================================\n\nclass CNNDMAdapter(DatasetAdapter):\n    \"\"\"Adapter for CNN/DailyMail summarization dataset from Kaggle\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"cnn_dailymail\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"CNN/DailyMail\")\n        self.paths = config[\"paths\"]\n        self.max_samples = config.get(\"max_samples\", None)\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load and merge all CNN/DM splits, with optional capping\"\"\"\n        train = pd.read_csv(self.paths[\"train\"])\n        val = pd.read_csv(self.paths[\"validation\"])\n        test = pd.read_csv(self.paths[\"test\"])\n        merged = pd.concat([train, val, test], ignore_index=True)\n        \n        # Cap if specified (summarization dataset is huge)\n        if self.max_samples and len(merged) > self.max_samples:\n            merged = merged.sample(self.max_samples, random_state=42)\n        \n        return merged\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform CNN/DM to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": f\"Summarize this article:\\n\\n{row['article']}\",\n                \"answer\": row['highlights'],\n                \"metadata\": {\n                    \"source\": self.source_name,\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: StrategyQA (Logic)\n# ============================================================\n\nclass StrategyQAAdapter(DatasetAdapter):\n    \"\"\"Adapter for StrategyQA logic dataset from HuggingFace\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"strategy_qa\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"StrategyQA\")\n        self.path = config[\"path\"]\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load StrategyQA from disk\"\"\"\n        from datasets import load_from_disk\n        dataset = load_from_disk(self.path)\n        # Merge all splits if multiple exist\n        if hasattr(dataset, 'keys'):\n            all_data = []\n            for split in dataset.keys():\n                all_data.append(dataset[split].to_pandas())\n            return pd.concat(all_data, ignore_index=True)\n        return dataset.to_pandas()\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform StrategyQA to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            # Convert boolean to Yes/No\n            answer = \"Yes\" if row.get('answer', False) else \"No\"\n            \n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": row['question'],\n                \"answer\": answer,\n                \"metadata\": {\n                    \"source\": self.source_name,\n                    \"decomposition\": row.get('decomposition', []),\n                    \"facts\": row.get('facts', []),\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: WritingPrompts (Creative Writing)\n# ============================================================\n\nclass WritingPromptsAdapter(DatasetAdapter):\n    \"\"\"Adapter for WritingPrompts creative writing dataset from HuggingFace\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"writing_prompts\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"WritingPrompts\")\n        self.path = config[\"path\"]\n        self.max_samples = 6000  # ADD THIS - cap to 6K\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load WritingPrompts from disk\"\"\"\n        from datasets import load_from_disk\n        dataset = load_from_disk(self.path)\n        # Usually has 'train' split\n        if hasattr(dataset, 'keys'):\n            df = dataset['train'].to_pandas()\n        else:\n            df = dataset.to_pandas()\n        \n        # ADD THIS - Cap if too large\n        if self.max_samples and len(df) > self.max_samples:\n            df = df.sample(self.max_samples, random_state=42)\n        \n        return df\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform WritingPrompts to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            # Remove [WP], [TT], etc tags from prompt\n            prompt = row['prompt']\n            for tag in ['[WP]', '[TT]', '[FF]', '[EU]', '[PI]']:\n                prompt = prompt.replace(tag, '').strip()\n            \n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": prompt,\n                \"answer\": row['story'],\n                \"metadata\": {\n                    \"source\": self.source_name,\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: Dolly (Creative Ideation)\n# ============================================================\n\nclass DollyAdapter(DatasetAdapter):\n    \"\"\"Adapter for Dolly brainstorming dataset from HuggingFace\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"dolly\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"Dolly-15k\")\n        self.path = config[\"path\"]\n        self.filter_category = config.get(\"filter_category\")\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load Dolly and filter to brainstorming category\"\"\"\n        from datasets import load_from_disk\n        dataset = load_from_disk(self.path)\n        df = dataset['train'].to_pandas() if hasattr(dataset, 'keys') else dataset.to_pandas()\n        \n        # Filter to brainstorming only\n        if self.filter_category and 'category' in df.columns:\n            df = df[df['category'] == self.filter_category]\n        \n        return df\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform Dolly to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": row['instruction'],\n                \"answer\": row['response'],\n                \"metadata\": {\n                    \"source\": self.source_name,\n                    \"category\": row.get('category', ''),\n                    \"context\": row.get('context', ''),\n                }\n            })\n        return data\n\n\n# ============================================================\n# CONCRETE ADAPTER: No Robots (Creative Ideation)\n# ============================================================\n\nclass NoRobotsAdapter(DatasetAdapter):\n    \"\"\"Adapter for No Robots brainstorming dataset from HuggingFace\"\"\"\n    \n    def __init__(self):\n        config = DATASET_CONFIG[\"no_robots\"]\n        super().__init__(domain=config[\"domain\"], source_name=\"NoRobots\")\n        self.path = config[\"path\"]\n        self.filter_category = config.get(\"filter_category\")\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load No Robots and filter to brainstorming category\"\"\"\n        from datasets import load_from_disk\n        dataset = load_from_disk(self.path)\n        \n        # Merge train_sft and test_sft\n        all_data = []\n        for split in dataset.keys():\n            all_data.append(dataset[split].to_pandas())\n        df = pd.concat(all_data, ignore_index=True)\n        \n        # Filter to brainstorming only\n        if self.filter_category and 'category' in df.columns:\n            df = df[df['category'] == self.filter_category]\n        \n        return df\n    \n    def transform(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Transform No Robots to unified schema\"\"\"\n        data = []\n        for _, row in df.iterrows():\n            # Extract user prompt from messages\n            prompt = str(row['prompt'])\n            \n            # Extract assistant response from messages\n            messages = row.get('messages', [])\n            answer = \"\"\n            \n            # Handle messages - convert to list if numpy array\n            if hasattr(messages, 'tolist'):\n                messages = messages.tolist()\n            \n            # Ensure messages is iterable\n            if isinstance(messages, (list, tuple)):\n                for msg in messages:\n                    if isinstance(msg, dict) and msg.get('role') == 'assistant':\n                        answer = str(msg.get('content', ''))\n                        break\n            \n            data.append({\n                \"domain\": self.domain,\n                \"prompt\": prompt,\n                \"answer\": answer,\n                \"metadata\": {\n                    \"source\": self.source_name,\n                    \"category\": str(row.get('category', '')),\n                }\n            })\n        return data\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Mixer","metadata":{}},{"cell_type":"code","source":"class DatasetMixer:\n    \"\"\"Combines all dataset adapters into full pool\"\"\"\n    \n    def __init__(self):\n        self.adapters = [\n            GSM8KAdapter(),\n            MBPPAdapter(),\n            SciQAdapter(),\n            CNNDMAdapter(),\n            StrategyQAAdapter(),\n            WritingPromptsAdapter(),\n            DollyAdapter(),\n            NoRobotsAdapter(),\n        ]\n    \n    def create_full_pool(self) -> List[Dict]:\n        \"\"\"Process all adapters and combine into single pool\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"CREATING FULL DATASET POOL\")\n        print(\"=\"*60)\n        \n        all_data = []\n        \n        for adapter in self.adapters:\n            try:\n                data = adapter.process()\n                all_data.extend(data)\n            except Exception as e:\n                print(f\"\\nERROR processing {adapter.source_name}: {e}\")\n                continue\n        \n        # Shuffle\n        import random\n        random.seed(42)\n        random.shuffle(all_data)\n        \n        return all_data\n    \n    def save_pool(self, data: List[Dict], output_path: str):\n        \"\"\"Save pool as JSONL\"\"\"\n        with open(output_path, 'w') as f:\n            for item in data:\n                # Convert numpy/pandas objects to JSON-serializable types\n                serializable_item = to_json_serializable(item)\n                f.write(json.dumps(serializable_item) + '\\n')\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"SAVED POOL\")\n        print(f\"{'='*60}\")\n        print(f\"Location: {output_path}\")\n        print(f\"Total samples: {len(data)}\")\n        \n        # Print domain distribution\n        domain_counts = Counter(item['domain'] for item in data)\n        print(f\"\\nðŸ“Š Domain Distribution:\")\n        for domain in sorted(domain_counts.keys()):\n            count = domain_counts[domain]\n            pct = (count / len(data)) * 100\n            print(f\"  {domain:20s}: {count:5d} ({pct:5.1f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Join Datasets","metadata":{}},{"cell_type":"code","source":"print(\"TESTING FULL DATASET PIPELINE - ALL 8 ADAPTERS\")\n\n# Create mixer\nmixer = DatasetMixer()\n\n# Create full pool\nfull_pool = mixer.create_full_pool()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Dataset","metadata":{}},{"cell_type":"code","source":"# Save\noutput_path = '/kaggle/working/full_dataset_pool.jsonl'\nmixer.save_pool(full_pool, output_path)\n\n# Save sample for inspection\nsample_output = full_pool[:20]\nwith open('/kaggle/working/sample_pool.jsonl', 'w') as f:\n    for item in sample_output:\n        serializable_item = to_json_serializable(item)\n        f.write(json.dumps(serializable_item) + '\\n')\n\nprint(f\"\\nSaved 20 samples to /kaggle/working/sample_pool.jsonl\")\nprint(f\"\\nPIPELINE COMPLETE!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing & Validation","metadata":{}},{"cell_type":"code","source":"import json\nmetadata = {\n    \"title\": \"Temporal Flux Calibration Dataset v2\",\n    \"id\": \"fissalalsharef/temporal-flux-calibration-v2\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\nwith open('/kaggle/working/dataset-metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(\"Metadata file created\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find the truncated question in DataFrame if any.\ndf[df['prompt'].str.contains('Hani said she would do 3 more situps per', na=False)]['prompt'].values[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}