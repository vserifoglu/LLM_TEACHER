{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":14291659,"sourceType":"datasetVersion","datasetId":9122651},{"sourceId":14304415,"sourceType":"datasetVersion","datasetId":9131270,"isSourceIdPinned":false},{"sourceId":14395045,"sourceType":"datasetVersion","datasetId":9193428}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Domain GRPO Training\n\n**Goal:** Train Gemma 3 1B with GRPO on 15K multi-domain samples\n\n**Domains:** Math, Coding, Science, Logic, Summarization, Creative Writing, Creative Ideation","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Clean up\n!pip uninstall -q -y gensim bigframes tensorflow-decision-forests tf-keras flax jax jaxlib qwix tunix\n\n# Install Google Cloud SDKs\n!pip install -U -q google-cloud-storage google-cloud-automl google-cloud-bigquery protobuf\n\n# Install NumPy 2.0\n!pip install -q \"numpy>=2.0\" \"ml_dtypes>=0.4.0\"\n\n# Install EXACT working versions\n!pip install -q \\\n    \"jax[tpu]==0.8.1\" \\\n    \"flax==0.12.1\" \\\n    \"qwix==0.1.4\" \\\n    \"optax==0.2.6\" \\\n    \"orbax-checkpoint==0.11.31\" \\\n    \"chex==0.1.91\" \\\n    \"google-tunix[prod]==0.1.3\" \\\n    tensorflow \\\n    kagglehub \\\n    grain \\\n    humanize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:49:58.368237Z","iopub.execute_input":"2025-12-31T00:49:58.368402Z","iopub.status.idle":"2025-12-31T00:49:58.396259Z","shell.execute_reply.started":"2025-12-31T00:49:58.368385Z","shell.execute_reply":"2025-12-31T00:49:58.395529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nimport re\nimport csv\nimport shutil\nfrom pathlib import Path\nfrom pprint import pprint\nfrom tqdm import tqdm\n\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nfrom flax import nnx\nimport grain\nimport optax\nimport humanize\nfrom orbax import checkpoint as ocp\n\n# Tunix imports\nfrom tunix.models.gemma3 import model, params\nfrom tunix.generate import sampler as sampler_lib\n\n# GRPO-specific imports\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.rl.grpo.grpo_learner import GRPOLearner, GRPOConfig\nfrom tunix.sft import metrics_logger\n\nimport qwix\nimport numpy as np \nimport json\nimport pandas as pd\nimport re\nfrom typing import Optional, Dict, List, Tuple\nfrom abc import ABC, abstractmethod\nimport sys\nfrom typing import Dict, List\nimport pandas as pd\nimport grain\nfrom typing import Dict, Tuple, Optional, List\nfrom dataclasses import dataclass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:49:58.396709Z","iopub.execute_input":"2025-12-31T00:49:58.396876Z","iopub.status.idle":"2025-12-31T00:50:01.924298Z","shell.execute_reply.started":"2025-12-31T00:49:58.396861Z","shell.execute_reply":"2025-12-31T00:50:01.923172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration & HyperParams","metadata":{}},{"cell_type":"code","source":"# ==================== PATHS ====================\nDATASET_PATH = \"/kaggle/input/harmonic-oscillation/full_dataset_pool.jsonl\"\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/grpo_checkpoints/\"\n\n# ==================== TRAINING DATA CONFIG ====================\nTRAIN_SIZE = 10000\nVAL_SIZE = 500\nBATCH_SIZE = 2\nDISCO_TEMP = 0.5\nSEED = 42\n\n# ==================== GRPO CONFIG ====================\nNUM_EPOCHS = 1\nNUM_ITERATIONS = 4\nNUM_GENERATIONS = 4\n\n# RL hyperparameters\nBETA = 0.04\nEPSILON = 0.2\n\n# Training config\nTRAIN_MICRO_BATCH_SIZE = 2\nMAX_STEPS = int((TRAIN_SIZE / BATCH_SIZE) * NUM_ITERATIONS * NUM_EPOCHS)\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\nMAX_GRAD_NORM = 0.1\n\n# Checkpointing\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 3\nEVAL_EVERY_N_STEPS = 500\n\n# ==================== MODEL CONFIG ====================\nMODEL_CP_PATH = params.GEMMA3_1B_IT\nMESH = ((1, 4), (\"fsdp\", \"tp\"))\n\n# LoRA config\nLORA_RANK = 16\nLORA_ALPHA = 32.0\n\n# ==================== GENERATION CONFIG ====================\nMAX_PROMPT_LENGTH = 1024          # Both need same limit\nTOTAL_GENERATION_STEPS = 512      # MUST MATCH OR INFERENCE >= TRAINING\nTEMPERATURE = 0.7\nINFERENCE_TEMPERATURE = 0.0\nTOP_P = 0.95\nTOP_K = 50\n\n# ==================== PROMPTING ====================\nSYSTEM_PROMPT = \"\"\"Provide your reasoning in <reasoning> tags, then your final answer in <answer> tags.\nFormat:\n<reasoning>Your step-by-step thinking</reasoning>\n<answer>Your final answer</answer>\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\nTask: {question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.924706Z","iopub.execute_input":"2025-12-31T00:50:01.925007Z","iopub.status.idle":"2025-12-31T00:50:01.929704Z","shell.execute_reply.started":"2025-12-31T00:50:01.924990Z","shell.execute_reply":"2025-12-31T00:50:01.928915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"class DatasetLoader:\n    \"\"\"Loads and preprocesses multi-domain JSONL dataset with DISCO sampling.\"\"\"\n    \n    def __init__(self, jsonl_path: str):\n        \"\"\"\n        Initialize loader.\n        \n        Args:\n            jsonl_path: Path to JSONL file with dataset pool\n        \"\"\"\n        self.jsonl_path = jsonl_path\n        self.df = None\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load JSONL file into DataFrame.\"\"\"\n        print(f\"Loading dataset from {self.jsonl_path}...\")\n        \n        self.df = pd.read_json(self.jsonl_path, lines=True)\n        \n        # print(f\"Loaded {len(self.df)} total samples\")\n        # print(f\"Domains: {self.df['domain'].unique()}\")\n        # print(\"\\nDomain distribution:\")\n        # for domain, count in self.df['domain'].value_counts().items():\n        #     pct = (count / len(self.df)) * 100\n        #     print(f\"  {domain:20s}: {count:6d} ({pct:5.1f}%)\")\n        \n        return self.df\n    \n    def compute_disco_proportions(self, temperature: float = 1.0) -> Dict[str, float]:\n        \"\"\"\n        Compute DISCO-adjusted domain proportions.\n        \n        Args:\n            temperature: DISCO temperature\n                - T=0.5: Moderate balancin\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Get natural proportions\n        domain_counts = self.df['domain'].value_counts()\n        total = len(self.df)\n        natural_props = {domain: count / total for domain, count in domain_counts.items()}\n        \n        # Apply DISCO temperature\n        if temperature == 1.0:\n            # Natural distribution\n            adjusted = natural_props\n        else:\n            # Temperature-adjusted distribution\n            adjusted_unnormalized = {\n                domain: prop ** temperature\n                for domain, prop in natural_props.items()\n            }\n            \n            # Normalize to sum to 1.0\n            total_adjusted = sum(adjusted_unnormalized.values())\n            adjusted = {\n                domain: val / total_adjusted\n                for domain, val in adjusted_unnormalized.items()\n            }\n        \n        # ---- DEBUGGIN: Print comparison ---- \n        # print(f\"\\nDISCO Proportions (T={temperature}):\")\n        # print(f\"{'Domain':<20} {'Natural':>10} {'DISCO':>10} {'Change':>10}\")\n        # print(\"-\" * 52)\n        # for domain in sorted(natural_props.keys()):\n        #     nat = natural_props[domain]\n        #     disco = adjusted[domain]\n        #     change = ((disco - nat) / nat) * 100\n        #     print(f\"{domain:<20} {nat:>9.1%} {disco:>9.1%} {change:>+9.1f}%\")\n        \n        return adjusted\n    \n    def sample_dataset(\n        self,\n        total_size: int,\n        temperature: float,\n        seed: int\n    ) -> pd.DataFrame:\n        \"\"\"\n        Sample dataset using DISCO proportions.\n        \n        Args:\n            total_size: Total number of samples to draw\n            temperature: DISCO temperature 0.5\n            seed: Random seed for reproducibility\n        \n        Returns:\n            DataFrame with sampled data\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Compute DISCO proportions\n        proportions = self.compute_disco_proportions(temperature)\n        \n        # Sample from each domain\n        sampled_dfs = []\n        \n        print(f\"\\nSampling {total_size} samples:\")\n        for domain, proportion in proportions.items():\n            target_count = int(total_size * proportion)\n            domain_df = self.df[self.df['domain'] == domain]\n            available = len(domain_df)\n            \n            # Check if we have enough samples\n            if target_count > available:\n                sampled = domain_df\n            else:\n                sampled = domain_df.sample(n=target_count, random_state=seed)\n            \n            sampled_dfs.append(sampled)\n            print(f\"  {domain:<20}: Sampled {len(sampled):4d} samples\")\n        \n        # Combine and shuffle\n        combined = pd.concat(sampled_dfs, ignore_index=True)\n        combined = combined.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        \n        print(f\"\\nTotal: {len(combined)} samples\")\n        return combined\n    \n    def create_datasets(\n        self,\n        train_size: int,\n        temperature: float,\n        batch_size: int,\n        seed: int,\n        val_size: Optional[int] = None\n    ) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n        \"\"\"\n        Create train and validation Grain datasets.\n        \n        Args:\n            train_size: Number of training samples\n            val_size: Number of validation samples (optional)\n            temperature: DISCO temperature\n            batch_size: Batch size\n            seed: Random seed\n        \n        Returns:\n            (train_dataset, val_dataset) as Grain datasets\n        \"\"\"\n        # Sample training data\n        print(\"=\"*60)\n        print(\"TRAINING DATA\")\n        print(\"=\"*60)\n        train_df = self.sample_dataset(\n            total_size=train_size,\n            temperature=temperature,\n            seed=seed\n        )\n        \n        train_dataset = self._to_grain_dataset(train_df, batch_size, shuffle=True, seed=seed)\n        \n        # Validation dataset (optional)\n        val_dataset = None\n        if val_size:\n            print(\"\\n\" + \"=\"*60)\n            print(\"VALIDATION DATA\")\n            print(\"=\"*60)\n            val_df = self.sample_dataset(\n                total_size=val_size,\n                temperature=temperature,\n                seed=seed + 1  # Different seed\n            )\n            val_dataset = self._to_grain_dataset(val_df, batch_size, seed, shuffle=False)\n        \n        return train_dataset, val_dataset\n    \n    def _to_grain_dataset(\n        self,\n        df: pd.DataFrame,\n        batch_size: int,\n        seed: int,\n        shuffle: bool = True,\n    ) -> grain.MapDataset:\n        \"\"\"Convert DataFrame to batched Grain dataset.\"\"\"\n        # Convert to list of dicts\n        data = df.to_dict('records')\n        \n        # Create Grain dataset\n        dataset = grain.MapDataset.source(data)\n        \n        if shuffle:\n            dataset = dataset.shuffle(seed=seed)\n        \n        # Map to GRPO format (includes truncation!)\n        dataset = dataset.map(self._format_for_grpo)\n        \n        # Batch\n        dataset = dataset.batch(batch_size)\n        \n        return dataset\n    \n    def _format_for_grpo(self, item: Dict) -> Dict:\n        \"\"\"\n        Format a single item for GRPO training.\n        \n        Truncates very long prompts to fit within token limits.\n        \"\"\"\n        import json\n        \n        # Truncate long prompts\n        prompt_text = item['prompt']\n        MAX_CHARS = 2500  # ~625 tokens (safe for 1024 limit)\n        \n        if len(prompt_text) > MAX_CHARS:\n            prompt_text = prompt_text[:MAX_CHARS] + \"...\"\n        \n        # Format prompt\n        formatted_prompt = TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=prompt_text\n        )\n        \n        # Normalize metadata\n        metadata = item.get('metadata', {})\n        if not isinstance(metadata, dict):\n            metadata = {}\n        \n        return {\n            \"prompts\": formatted_prompt,\n            \"domain\": item['domain'],\n            \"question\": item['prompt'],\n            \"answer\": item['answer'],\n            \"metadata_str\": json.dumps(metadata),\n        }\n\n\n# ============================================================================\n# CONVENIENCE FUNCTIONS\n# ============================================================================\n\ndef load_dataset_for_training(\n    jsonl_path: str,\n    train_size: int,\n    batch_size: int,\n    disco_temperature: float,\n    seed: int = 42,\n    val_size: Optional[int] = None\n) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n    \"\"\"\n    Convenience function to load dataset in one call.\n    \n    Args:\n        jsonl_path: Path to JSONL dataset\n        train_size: Number of training samples\n        val_size: Number of validation samples (None to skip)\n        batch_size: Batch size\n        disco_temperature: DISCO temperature\n            - 1.0 = Natural distribution\n            - 0.5 = Moderate balancing (recommended)\n            - 0.3 = Aggressive balancing\n        seed: Random seed\n    \n    Returns:\n        (train_dataset, val_dataset)\n    \n    Example:\n        train_ds, val_ds = load_dataset_for_training(\n            '/kaggle/input/dataset/pool.jsonl',\n            train_size=15000,\n            disco_temperature=0.5\n        )\n    \"\"\"\n    loader = DatasetLoader(jsonl_path)\n    return loader.create_datasets(\n        train_size=train_size,\n        val_size=val_size,\n        temperature=disco_temperature,\n        batch_size=batch_size,\n        seed=seed\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.930506Z","iopub.execute_input":"2025-12-31T00:50:01.930684Z","iopub.status.idle":"2025-12-31T00:50:01.956142Z","shell.execute_reply.started":"2025-12-31T00:50:01.930669Z","shell.execute_reply":"2025-12-31T00:50:01.955449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validators","metadata":{}},{"cell_type":"code","source":"class XMLValidator:\n    \"\"\"Validates XML structure with strict ordering and uniqueness checks.\"\"\"\n    \n    REASONING_START = \"<reasoning>\"\n    REASONING_END = \"</reasoning>\"\n    ANSWER_START = \"<answer>\"\n    ANSWER_END = \"</answer>\"\n    \n    @classmethod\n    def is_valid(cls, response: str) -> bool:\n        \"\"\"\n        Check if response has valid XML structure.\n        \n        Requirements:\n        1. Exactly ONE <reasoning> tag pair\n        2. Exactly ONE <answer> tag pair\n        3. <reasoning> appears BEFORE <answer>\n        4. No overlapping/nested tags\n        5. Content not empty\n        \n        Returns:\n            bool: True if valid, False otherwise\n        \"\"\"\n        # Check tag counts\n        if response.count(cls.REASONING_START) != 1 or response.count(cls.REASONING_END) != 1:\n            return False\n        if response.count(cls.ANSWER_START) != 1 or response.count(cls.ANSWER_END) != 1:\n            return False\n        \n        # Check order\n        reasoning_start_pos = response.find(cls.REASONING_START)\n        reasoning_end_pos = response.find(cls.REASONING_END)\n        answer_start_pos = response.find(cls.ANSWER_START)\n        answer_end_pos = response.find(cls.ANSWER_END)\n        \n        # Reasoning must come before answer\n        if reasoning_start_pos >= answer_start_pos:\n            return False\n        \n        # Tags must be properly paired (start before end)\n        if reasoning_start_pos >= reasoning_end_pos:\n            return False\n        if answer_start_pos >= answer_end_pos:\n            return False\n        \n        # No overlapping (reasoning must fully end before answer starts)\n        if reasoning_end_pos >= answer_start_pos:\n            return False\n        \n        # Extract content and check not empty\n        reasoning = cls.extract_reasoning(response)\n        answer = cls.extract_answer(response)\n        \n        if not reasoning or not answer:\n            return False\n        if len(reasoning.strip()) == 0 or len(answer.strip()) == 0:\n            return False\n        \n        return True\n    \n    @classmethod\n    def extract_reasoning(cls, response: str) -> Optional[str]:\n        \"\"\"Extract reasoning content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.REASONING_START)}(.*?){re.escape(cls.REASONING_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n    \n    @classmethod\n    def extract_answer(cls, response: str) -> Optional[str]:\n        \"\"\"Extract answer content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.ANSWER_START)}(.*?){re.escape(cls.ANSWER_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-31T00:50:01.956570Z","iopub.execute_input":"2025-12-31T00:50:01.956725Z","iopub.status.idle":"2025-12-31T00:50:01.971325Z","shell.execute_reply.started":"2025-12-31T00:50:01.956710Z","shell.execute_reply":"2025-12-31T00:50:01.970444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DomainValidator(ABC):\n    \"\"\"Abstract base class for domain-specific answer validation.\"\"\"\n    \n    @abstractmethod\n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Check if predicted answer matches ground truth.\"\"\"\n        pass\n\n\nclass MathValidator(DomainValidator):\n    \"\"\"Validator for math domain - extracts number after ####.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Math answers are in format: \"reasoning\\n#### 60\"\n        Extract number after #### and compare.\n        \"\"\"\n        # Extract ground truth number\n        if \"####\" in ground_truth:\n            gt_value = ground_truth.split(\"####\")[1].strip()\n        else:\n            gt_value = ground_truth.strip()\n        \n        # Normalize both to float for comparison\n        try:\n            gt_num = float(self._normalize_number(gt_value))\n            pred_num = float(self._normalize_number(predicted))\n            return abs(gt_num - pred_num) < 1e-6  # Float comparison tolerance\n        except (ValueError, AttributeError):\n            return False\n    \n    @staticmethod\n    def _normalize_number(text: str) -> str:\n        \"\"\"Extract and normalize numeric value.\"\"\"\n        # Remove common formatting: commas, dollar signs, percent\n        cleaned = text.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        # Extract first number (handles cases like \"answer is 42\")\n        numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n        return numbers[0] if numbers else text\n\n\nclass CodingValidator(DomainValidator):\n    \"\"\"Validator for coding domain - executes test cases.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Execute test cases from metadata.\n        Returns True only if ALL test cases pass.\n        \"\"\"\n        if not metadata or 'test_cases' not in metadata:\n            return False\n        \n        test_cases = metadata['test_cases']\n        \n        try:\n            # Create namespace with the predicted code\n            namespace = {}\n            exec(predicted, namespace)\n            \n            # Run each test case\n            for test in test_cases:\n                try:\n                    exec(test, namespace)\n                except AssertionError:\n                    return False  # Test failed\n                except Exception:\n                    return False  # Execution error\n            \n            return True  # All tests passed\n        except Exception:\n            return False  # Code doesn't execute\n\n\nclass ScienceValidator(DomainValidator):\n    \"\"\"Validator for science domain - case-insensitive exact match.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Case-insensitive comparison after normalization.\"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        return pred_normalized == gt_normalized\n\n\nclass LogicValidator(DomainValidator):\n    \"\"\"Validator for logic domain - Yes/No normalization.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Normalize Yes/No answers.\n        Handles: yes, Yes, YES, no, No, NO\n        \"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        \n        # Check for yes/no presence\n        pred_is_yes = \"yes\" in pred_normalized\n        pred_is_no = \"no\" in pred_normalized\n        gt_is_yes = \"yes\" in gt_normalized\n        gt_is_no = \"no\" in gt_normalized\n        \n        # Match if both have same yes/no\n        return (pred_is_yes and gt_is_yes) or (pred_is_no and gt_is_no)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.971694Z","iopub.execute_input":"2025-12-31T00:50:01.971846Z","iopub.status.idle":"2025-12-31T00:50:01.986019Z","shell.execute_reply.started":"2025-12-31T00:50:01.971831Z","shell.execute_reply":"2025-12-31T00:50:01.985193Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reward Func","metadata":{}},{"cell_type":"code","source":"class HeuristicRewards:\n    \"\"\"Heuristic-based quality rewards for creative domains.\"\"\"\n    \n    @staticmethod\n    def length_score(text: str, target: int, min_len: int, max_len: int) -> float:\n        \"\"\"\n        Score based on length appropriateness.\n        Returns 1.0 if within [min_len, max_len], decays outside.\n        \"\"\"\n        length = len(text.split())\n        \n        if min_len <= length <= max_len:\n            return 1.0\n        else:\n            # Linear decay based on distance from range\n            if length < min_len:\n                distance = min_len - length\n                max_distance = min_len\n            else:  # length > max_len\n                distance = length - max_len\n                max_distance = target\n            \n            return max(0.0, 1.0 - distance / max_distance)\n    \n    @staticmethod\n    def lexical_diversity(text: str) -> float:\n        \"\"\"\n        Calculate lexical diversity: unique words / total words.\n        \"\"\"\n        words = text.lower().split()\n        if len(words) == 0:\n            return 0.0\n        \n        unique_words = len(set(words))\n        return unique_words / len(words)\n    \n    @staticmethod\n    def prompt_relevance(prompt: str, reasoning: str) -> float:\n        \"\"\"\n        Calculate relevance by keyword overlap between prompt and reasoning.\n        \"\"\"\n        # Extract meaningful words (>3 chars, not common stop words)\n        stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', \n                     'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him',\n                     'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'way',\n                     'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}\n        \n        prompt_words = set(w.lower() for w in prompt.split() if len(w) > 3 and w.lower() not in stop_words)\n        reasoning_words = set(w.lower() for w in reasoning.split() if len(w) > 3 and w.lower() not in stop_words)\n        \n        if len(prompt_words) == 0:\n            return 0.5  # Default score if no meaningful words in prompt\n        \n        overlap = len(prompt_words & reasoning_words)\n        return overlap / len(prompt_words)\n\n\n# ============================================================================\n# MAIN REWARD CALCULATOR\n# ============================================================================\n\nclass RewardCalculator:\n    \"\"\"\n    Main reward calculator that routes to appropriate validators.\n    \n    Reward breakdown:\n    - Format: 0.2 (all domains)\n    - Verifiable: 0.6 correctness + 0.2 bonus\n    - Creative: 0.3 length + 0.25 diversity + 0.25 relevance\n    \"\"\"\n    \n    VERIFIABLE_DOMAINS = {\"math\", \"coding\", \"science\", \"logic\"}\n    CREATIVE_DOMAINS = {\"creative_writing\", \"creative_ideation\", \"summarization\"}\n    \n    def __init__(self):\n        \"\"\"Initialize validators.\"\"\"\n        self.xml_validator = XMLValidator()\n        self.validators = {\n            \"math\": MathValidator(),\n            \"coding\": CodingValidator(),\n            \"science\": ScienceValidator(),\n            \"logic\": LogicValidator(),\n        }\n        self.heuristics = HeuristicRewards()\n    \n    def compute_reward(\n        self,\n        domain: str,\n        prompt: str,\n        response: str,\n        ground_truth: Optional[str] = None,\n        metadata: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Compute total reward for a response.\n        \n        Args:\n            domain: Task domain (math, coding, science, etc.)\n            prompt: Original prompt/question\n            response: Model's generated response\n            ground_truth: Expected answer (for verifiable domains)\n            metadata: Additional data (e.g., test_cases for coding)\n        \n        Returns:\n            float: Total reward score [0.0, 1.0]\n        \"\"\"\n        # HARD DEPENDENCY: Format must be valid\n        if not self.xml_validator.is_valid(response):\n            return 0.0\n        \n        # Format is valid - start with format reward\n        reward = 0.2\n        \n        # Extract content\n        reasoning = self.xml_validator.extract_reasoning(response)\n        answer = self.xml_validator.extract_answer(response)\n        \n        # Domain-specific rewards\n        if domain in self.VERIFIABLE_DOMAINS:\n            reward += self._compute_verifiable_reward(\n                domain, answer, ground_truth, metadata\n            )\n        elif domain in self.CREATIVE_DOMAINS:\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        else:\n            # Unknown domain - use creative heuristics as fallback\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        \n        return min(reward, 1.0)  # Cap at 1.0\n    \n    def _compute_verifiable_reward(\n        self,\n        domain: str,\n        answer: str,\n        ground_truth: str,\n        metadata: Optional[Dict]\n    ) -> float:\n        \"\"\"Compute reward for verifiable domains.\"\"\"\n        validator = self.validators[domain]\n        \n        # Correctness check (0.6 points)\n        if validator.is_correct(answer, ground_truth, metadata):\n            return 0.8  # 0.6 for correctness + 0.2 bonus\n        else:\n            return 0.0\n    \n    def _compute_creative_reward(\n        self,\n        prompt: str,\n        reasoning: str,\n        answer: str\n    ) -> float:\n        \"\"\"Compute reward for creative domains using heuristics.\"\"\"\n        score = 0.0\n        \n        # Length appropriateness (0.3 points)\n        reasoning_score = self.heuristics.length_score(\n            reasoning, target=250, min_len=20, max_len=500\n        )\n        answer_score = self.heuristics.length_score(\n            answer, target=150, min_len=10, max_len=300\n        )\n        score += 0.15 * reasoning_score\n        score += 0.15 * answer_score\n        \n        # Lexical diversity (0.25 points)\n        diversity = self.heuristics.lexical_diversity(answer)\n        score += 0.25 * diversity\n        \n        # Prompt relevance (0.25 points)\n        relevance = self.heuristics.prompt_relevance(prompt, reasoning)\n        score += 0.25 * relevance\n        \n        return score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.986490Z","iopub.execute_input":"2025-12-31T00:50:01.986646Z","iopub.status.idle":"2025-12-31T00:50:02.002826Z","shell.execute_reply.started":"2025-12-31T00:50:01.986632Z","shell.execute_reply":"2025-12-31T00:50:02.002146Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_reward_batch(\n    prompts: List[str],\n    completions: List[str],\n    domain: List[str] = None,\n    answer: List[str] = None,\n    metadata_str: List[str] = None,\n    **kwargs  # Catch any other fields\n) -> List[float]:\n    \"\"\"\n    Compute rewards for a batch of responses.\n    \n    Called by GRPO with:\n    - prompts: List of prompts\n    - completions: List of model responses\n    - **kwargs: Dict with domain, answer, metadata_str, etc.\n    \n    Returns:\n        List[float]: Reward scores for each response\n    \"\"\"\n    import json\n    \n    calculator = RewardCalculator()\n    \n    # Handle missing fields\n    if domain is None:\n        domain = [\"unknown\"] * len(completions)\n    if answer is None:\n        answer = [None] * len(completions)\n    if metadata_str is None:\n        metadata_str = [\"{}\"] * len(completions)\n    \n    # Parse metadata from JSON strings\n    metadatas = []\n    for meta_str in metadata_str:\n        try:\n            metadatas.append(json.loads(meta_str) if isinstance(meta_str, str) else {})\n        except:\n            metadatas.append({})\n    \n    # Compute rewards\n    rewards = []\n    for dom, prompt, completion, gt, meta in zip(\n        domain, prompts, completions, answer, metadatas\n    ):\n        reward = calculator.compute_reward(\n            domain=dom,\n            prompt=prompt,\n            response=completion,  # GRPO calls it completion\n            ground_truth=gt,\n            metadata=meta\n        )\n        rewards.append(reward)\n    \n    return rewards","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:02.003264Z","iopub.execute_input":"2025-12-31T00:50:02.003418Z","iopub.status.idle":"2025-12-31T00:50:02.016063Z","shell.execute_reply.started":"2025-12-31T00:50:02.003403Z","shell.execute_reply":"2025-12-31T00:50:02.015381Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Memory Utility","metadata":{}},{"cell_type":"code","source":"def show_hbm_usage():\n    \"\"\"Displays memory usage per device.\"\"\"\n    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n    for d in jax.local_devices():\n        stats = d.memory_stats()\n        used = stats[\"bytes_in_use\"]\n        limit = stats[\"bytes_limit\"]\n        print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:.1%}) on {d}\")\n\nshow_hbm_usage()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-31T00:50:02.016701Z","iopub.execute_input":"2025-12-31T00:50:02.016867Z","iopub.status.idle":"2025-12-31T00:50:12.118055Z","shell.execute_reply.started":"2025-12-31T00:50:02.016852Z","shell.execute_reply":"2025-12-31T00:50:12.116903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 6: Load Dataset","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"LOADING DATASET\")\nprint(\"=\"*60)\n\ntrain_dataset, val_dataset = load_dataset_for_training(\n    jsonl_path=DATASET_PATH,\n    train_size=TRAIN_SIZE,\n    val_size=VAL_SIZE,\n    batch_size=BATCH_SIZE,\n    disco_temperature=DISCO_TEMP,\n    seed=SEED\n)\n\nprint(f\"\\nâœ“ Train dataset: {type(train_dataset)}\")\nprint(f\"âœ“ Val dataset: {type(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:12.118807Z","iopub.execute_input":"2025-12-31T00:50:12.119003Z","iopub.status.idle":"2025-12-31T00:50:13.032538Z","shell.execute_reply.started":"2025-12-31T00:50:12.118986Z","shell.execute_reply":"2025-12-31T00:50:13.031494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 8: Load Base Model to Intermediate Checkpoint","metadata":{}},{"cell_type":"code","source":"try:\n    print(\"=\"*60)\n    print(\"LOADING BASE MODEL\")\n    print(\"=\"*60)\n    \n    # Clear any existing intermediate checkpoint\n    if os.path.exists(INTERMEDIATE_CKPT_DIR):\n        shutil.rmtree(INTERMEDIATE_CKPT_DIR)\n    os.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\n    os.makedirs(CKPT_DIR, exist_ok=True)\n    \n    # Load base Gemma model\n    config = model.ModelConfig.gemma3_1b()\n    gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n    tokenizer = params.create_tokenizer()\n    \n    # Save to intermediate checkpoint\n    checkpointer = ocp.StandardCheckpointer()\n    _, state = nnx.split(gemma)\n    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n    checkpointer.wait_until_finished()\n    \n    # Free memory\n    del gemma\n    del state\n    gc.collect()\n    \n    print(\"âœ“ Base model saved to intermediate checkpoint\")\n    # show_hbm_usage()\nexcept:pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:13.033171Z","iopub.execute_input":"2025-12-31T00:50:13.033361Z","iopub.status.idle":"2025-12-31T00:50:57.782865Z","shell.execute_reply.started":"2025-12-31T00:50:13.033345Z","shell.execute_reply":"2025-12-31T00:50:57.781574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 9: Create Reference and Policy Models","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING REFERENCE AND POLICY MODELS\")\nprint(\"=\"*60)\n\ndef get_gemma_ref_model(ckpt_path):\n    \"\"\"Load Gemma model with proper sharding.\"\"\"\n    mesh = jax.make_mesh(*MESH)\n    model_config = model.ModelConfig.gemma3_1b()\n    \n    # Create abstract model for shape inference\n    abs_gemma = nnx.eval_shape(\n        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, model_config)\n    )\n    \n    # Create sharded state specification\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    \n    # Restore checkpoint\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n    \n    # Merge graph and params\n    graph_def, _ = nnx.split(abs_gemma)\n    gemma = nnx.merge(graph_def, restored_params)\n    \n    return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n    \"\"\"Apply LoRA adapters to the model.\"\"\"\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=LORA_RANK,\n        alpha=LORA_ALPHA,\n    )\n    \n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    \n    # Apply sharding constraints\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    \n    return lora_model\n\n\n# Create reference model (frozen, for KL penalty)\nref_model, mesh, model_config = get_gemma_ref_model(\n    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n)\nprint(\"âœ“ Reference model loaded\")\n\n# Create policy model (will be trained with GRPO)\nlora_policy = get_lora_model(ref_model, mesh=mesh)\nprint(\"âœ“ Policy model with LoRA created\")\n\n# show_hbm_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:58:35.556490Z","iopub.execute_input":"2025-12-31T00:58:35.556834Z","iopub.status.idle":"2025-12-31T00:58:54.722362Z","shell.execute_reply.started":"2025-12-31T00:58:35.556813Z","shell.execute_reply":"2025-12-31T00:58:54.721151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 10: Create Optimizer","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING OPTIMIZER\")\nprint(\"=\"*60)\n\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\n\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n        optimizer,\n    )\n\nprint(f\"âœ“ AdamW optimizer with warmup cosine decay\")\nprint(f\"  - Peak LR: {LEARNING_RATE}\")\nprint(f\"  - Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  - Grad clip norm: {MAX_GRAD_NORM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:02.030902Z","iopub.execute_input":"2025-12-31T00:59:02.031180Z","iopub.status.idle":"2025-12-31T00:59:02.036240Z","shell.execute_reply.started":"2025-12-31T00:59:02.031159Z","shell.execute_reply":"2025-12-31T00:59:02.035222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 11: Configure GRPO Training","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CONFIGURING GRPO TRAINING\")\nprint(\"=\"*60)\n\n# Checkpoint saving options\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, \n    max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logging\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tensorboard/grpo\", \n    flush_every_n_steps=20\n)\n\n# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=1536,  # 1024 + 256 + 256 buffer\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1, 106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\n\nprint(\"âœ“ GRPO configuration complete\")\nprint(f\"  - Group size: {NUM_GENERATIONS} generations per prompt\")\nprint(f\"  - Iterations: {NUM_ITERATIONS}\")\nprint(f\"  - KL beta: {BETA}\")\nprint(f\"  - Clip epsilon: {EPSILON}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:05.693672Z","iopub.execute_input":"2025-12-31T00:59:05.693940Z","iopub.status.idle":"2025-12-31T00:59:05.699946Z","shell.execute_reply.started":"2025-12-31T00:59:05.693920Z","shell.execute_reply":"2025-12-31T00:59:05.698974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 12: Create GRPO Trainer","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING GRPO TRAINER\")\nprint(\"=\"*60)\n\n# Create RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# Create GRPO trainer with our multi-domain reward function\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[compute_reward_batch],  # Our custom multi-domain reward!\n    grpo_config=grpo_config,\n)\n\nprint(\"âœ“ GRPO trainer created\")\nprint(f\"  - Actor: LoRA policy model\")\nprint(f\"  - Reference: Frozen base model\")\nprint(f\"  - Reward function: Multi-domain (7 domains)\")\n\n# show_hbm_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:09.710600Z","iopub.execute_input":"2025-12-31T00:59:09.710874Z","iopub.status.idle":"2025-12-31T00:59:11.619096Z","shell.execute_reply.started":"2025-12-31T00:59:09.710854Z","shell.execute_reply":"2025-12-31T00:59:11.618105Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 13: Run Training\n\n**This will take ~7 hours. Monitor the logs for:**\n- Average reward increasing\n- Format compliance >95%\n- No OOM errors","metadata":{}},{"cell_type":"code","source":"# print(\"=\"*80)\n# print(\"STARTING GRPO TRAINING\")\n# print(\"=\"*80)\n# print(f\"Training steps: {MAX_STEPS}\")\n# print(f\"Checkpoint interval: {SAVE_INTERVAL_STEPS} steps\")\n# print(f\"Estimated time: ~7 hours\")\n# print(\"=\"*80)\n\n# with mesh:\n#     grpo_trainer.train(train_dataset)\n\n# print(\"=\"*80)\n# print(\"TRAINING COMPLETE\")\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:20.030827Z","iopub.execute_input":"2025-12-31T00:59:20.031070Z","execution_failed":"2025-12-31T09:45:05.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 14: Load Best Checkpoint","metadata":{}},{"cell_type":"code","source":"# print(\"Loading latest checkpoint for evaluation...\")\n\n# # Find the latest checkpoint\n# actor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\n# latest_step = -1\n# if os.path.exists(actor_ckpt_dir):\n#     for item in os.listdir(actor_ckpt_dir):\n#         if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n#             step = int(item)\n#             if step > latest_step:\n#                 latest_step = step\n\n# if latest_step == -1:\n#     print(\"âš  No checkpoints found, using current model state\")\n# else:\n#     print(f\"Loading checkpoint from step {latest_step}...\")\n    \n#     trained_ckpt_path = os.path.join(CKPT_DIR, \"actor\", str(latest_step), \"model_params\")\n    \n#     abs_params = jax.tree.map(\n#         lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n#         nnx.state(lora_policy, nnx.LoRAParam),\n#     )\n#     checkpointer = ocp.StandardCheckpointer()\n#     trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n    \n#     nnx.update(\n#         lora_policy,\n#         jax.tree.map(\n#             lambda a, b: b,\n#             nnx.state(lora_policy, nnx.LoRAParam),\n#             trained_lora_params,\n#         ),\n#     )\n#     print(f\"âœ“ Loaded checkpoint from step {latest_step}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 15: Validation","metadata":{}},{"cell_type":"markdown","source":"## Cell 16: Save Final Model","metadata":{}},{"cell_type":"code","source":"# print(\"=\"*60)\n# print(\"SAVING FINAL MODEL\")\n# print(\"=\"*60)\n\n# # Save to /kaggle/working (persists after session)\n# final_ckpt_path = \"/kaggle/working/grpo_multi_domain_final\"\n\n# if os.path.exists(final_ckpt_path):\n#     shutil.rmtree(final_ckpt_path)\n\n# # Save LoRA parameters\n# abs_params = jax.tree.map(\n#     lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n#     nnx.state(lora_policy, nnx.LoRAParam),\n# )\n# checkpointer = ocp.StandardCheckpointer()\n# lora_params = nnx.state(lora_policy, nnx.LoRAParam)\n# checkpointer.save(final_ckpt_path, lora_params)\n# checkpointer.wait_until_finished()\n\n# print(f\"âœ“ Model saved to {final_ckpt_path}\")\n\n# # Create metadata for Kaggle dataset\n# import json\n# metadata = {\n#     \"title\": \"GRPO Multi-Domain Reasoning Model\",\n#     \"id\": \"vserifoglu/grpo-multi-domain-final\",\n# }\n\n# with open('/kaggle/working/dataset-metadata.json', 'w') as f:\n#     json.dump(metadata, f)\n\n# print(\"âœ“ Metadata created\")\n# print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n# print(\"\\nNext steps:\")\n# print(\"1. Save notebook version\")\n# print(\"2. Go to Output tab\")\n# print(\"3. Create new dataset from output\")\n# print(\"4. Use for competition submission\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"=\"*60)\n# print(\"SAVING FINAL MODEL\")\n# print(\"=\"*60)\n\n# # Save to /kaggle/working (persists after session)\n# final_ckpt_path = \"/kaggle/working/grpo_multi_domain_final\"\n\n# if os.path.exists(final_ckpt_path):\n#     shutil.rmtree(final_ckpt_path)\n\n# # Save LoRA parameters\n# abs_params = jax.tree.map(\n#     lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n#     nnx.state(lora_policy, nnx.LoRAParam),\n# )\n# checkpointer = ocp.StandardCheckpointer()\n# lora_params = nnx.state(lora_policy, nnx.LoRAParam)\n# checkpointer.save(final_ckpt_path, lora_params)\n# checkpointer.wait_until_finished()\n\n# print(f\"âœ“ Model saved to {final_ckpt_path}\")\n\n# # Upload to Kaggle Datasets (automatic!)\n# import kagglehub\n\n# DATASET_HANDLE = \"fissalalsharef/grpo-multi-domain-final_v2\"\n\n# print(f\"\\nUploading to Kaggle: {DATASET_HANDLE}\")\n# kagglehub.dataset_upload(\n#     handle=DATASET_HANDLE,\n#     local_dataset_dir=final_ckpt_path,\n#     version_notes=\"Multi-domain GRPO training with DISCO balancing - Gemma 3 1B + LoRA\"\n# )\n\n# print(\"âœ“ Dataset uploaded!\")\n# print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n# print(f\"\\nYour model is available at:\")\n# print(f\"https://www.kaggle.com/datasets/{DATASET_HANDLE}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## verify model if loadable","metadata":{}},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"PRODUCTION VALIDATION - MODEL SETUP\")\nprint(\"=\"*80)\n\n# Step 1: Download model from Kaggle\nprint(\"\\n1. Downloading trained model from Kaggle...\")\nimport kagglehub\n\nDATASET_HANDLE = \"fissalalsharef/grpo-multi-domain-final\"\ndownloaded_path = kagglehub.dataset_download(DATASET_HANDLE)\n\nprint(f\"âœ“ Dataset downloaded to: {downloaded_path}\")\n\n# Step 2: Load base Gemma model\nprint(\"\\n2. Loading base Gemma 3 1B model...\")\nverification_base = params.create_model_from_checkpoint(\n    MODEL_CP_PATH, \n    model.ModelConfig.gemma3_1b()\n)\nprint(\"âœ“ Base model loaded\")\n\n# Step 3: Create LoRA structure\nprint(\"\\n3. Creating LoRA model structure...\")\nverification_policy = get_lora_model(verification_base, mesh)\n\n# Step 4: Load trained LoRA checkpoint\nprint(f\"\\n4. Loading LoRA checkpoint from Kaggle...\")\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(verification_policy, nnx.LoRAParam),\n)\n\ncheckpointer = ocp.StandardCheckpointer()\nloaded_params = checkpointer.restore(\n    downloaded_path,\n    target=abs_params\n)\n\n# Step 5: Apply LoRA params to base model\nprint(\"\\n5. Applying LoRA parameters...\")\nnnx.update(\n    verification_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(verification_policy, nnx.LoRAParam),\n        loaded_params,\n    ),\n)\nprint(\"âœ“ LoRA applied successfully\")\n\n# Step 6: Create sampler for testing\nprint(\"\\n6. Creating sampler...\")\nverification_sampler = sampler_lib.Sampler(\n    transformer=verification_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=2048,  # 1024 + 600 for generation\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… MODEL LOADED & READY FOR VALIDATION!\")\nprint(\"=\"*80)\nprint(f\"âœ“ Model: {DATASET_HANDLE}\")\nprint(f\"âœ“ Sampler: verification_sampler\")\nprint(f\"âœ“ Cache size: 1624 tokens\")\nprint(\"\\nâ–¶ Run Cell 1 to begin validation tests...\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"PRODUCTION-GRADE MODEL VALIDATION\")\nprint(\"=\"*80)\nimport random\nimport json\nfrom collections import defaultdict\n# Sample diverse test set from validation data\nprint(\"\\n1. Sampling test data...\")\ntest_samples = []\nfor batch in val_dataset:\n    for i in range(len(batch['domain'])):\n        test_samples.append({\n            'domain': batch['domain'][i],\n            'question': batch['question'][i],\n            'answer': batch['answer'][i],\n        })\n    if len(test_samples) >= 200:\n        break\n# Stratify by domain for balanced testing\ndomain_groups = defaultdict(list)\nfor sample in test_samples:\n    # if len(sample['question']) > 4000:\n    #     continue\n    domain_groups[sample['domain']].append(sample)\n# Sample evenly per domain (aim for ~15 per domain)\nbalanced_test_set = []\nsamples_per_domain = 15\nfor domain, samples in domain_groups.items():\n    sample_count = min(len(samples), samples_per_domain)\n    balanced_test_set.extend(random.sample(samples, sample_count))\nprint(f\"\\nâœ“ Test set created: {len(balanced_test_set)} samples\")\nprint(\"\\nDomain distribution:\")\nfor domain in set(s['domain'] for s in balanced_test_set):\n    count = sum(1 for s in balanced_test_set if s['domain'] == domain)\n    print(f\"  {domain:20s}: {count:2d} samples\")\n# Add edge cases manually\nedge_cases = [\n    {\"domain\": \"edge_short\", \"question\": \"What?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_short\", \"question\": \"?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_ambiguous\", \"question\": \"What is it?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_ambiguous\", \"question\": \"Can you help?\", \"answer\": \"N/A\"},\n]\nprint(f\"\\nâœ“ Added {len(edge_cases)} edge cases\")\nprint(f\"\\nTotal test samples: {len(balanced_test_set) + len(edge_cases)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only_logic = []\n# for i in balanced_test_set:\n#     if i[\"domain\"] == \"math\":\n#         only_logic.append(i)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"TEST 1: FORMAT COMPLIANCE (with Truncation Detection)\")\nprint(\"=\"*80)\n\n\nMAX_TOKENS = 700  # Generous limit to reduce truncation\n\ndef analyze_format_failure(output, output_length, max_tokens):\n    \"\"\"Classify failure as truncation or true model failure\"\"\"\n    \n    has_reasoning_open = \"<reasoning>\" in output\n    has_reasoning_close = \"</reasoning>\" in output\n    has_answer_open = \"<answer>\" in output\n    has_answer_close = \"</answer>\" in output\n    \n    tags_present = sum([has_reasoning_open, has_reasoning_close, \n                        has_answer_open, has_answer_close])\n    \n    near_limit = output_length >= (max_tokens - 10)  # Within 10 tokens of limit\n    \n    # Classification logic\n    if not has_reasoning_open and not has_answer_open:\n        return \"TRUE_FAILURE\", \"Missing opening tags - model didn't learn format\"\n    \n    if near_limit and tags_present >= 2:\n        if not has_answer_close:\n            return \"TRUNCATION\", \"Hit token limit before closing </answer>\"\n        if not has_reasoning_close:\n            return \"TRUNCATION\", \"Hit token limit before closing </reasoning>\"\n    \n    if output_length < 100 and tags_present < 2:\n        return \"TRUE_FAILURE\", \"Short output with no format - model didn't try\"\n    \n    if tags_present >= 3 and not has_answer_close:\n        if near_limit:\n            return \"TRUNCATION\", \"Almost complete, hit limit\"\n        else:\n            return \"TRUE_FAILURE\", \"Had room but didn't close tags\"\n    \n    return \"TRUE_FAILURE\", \"Other format issue\"\n\n\ndef validate_format(output):\n    \"\"\"Check if format is valid\"\"\"\n    checks = {\n        'has_reasoning_open': '<reasoning>' in output,\n        'has_reasoning_close': '</reasoning>' in output,\n        'has_answer_open': '<answer>' in output,\n        'has_answer_close': '</answer>' in output,\n    }\n    \n    all_present = all(checks.values())\n    \n    if all_present:\n        r_open = output.find('<reasoning>')\n        r_close = output.find('</reasoning>')\n        a_open = output.find('<answer>')\n        a_close = output.find('</answer>')\n        \n        correct_order = (r_open < r_close < a_open < a_close)\n        checks['correct_order'] = correct_order\n        return correct_order, checks\n    \n    return False, checks\n\n\n# Run validation\nprint(f\"\\nTesting {len(balanced_test_set)} samples with {MAX_TOKENS} token limit...\")\nprint(\"(This may take 10-15 minutes)\\n\")\n\nresults = []\ntrue_failures = []\ntruncation_failures = []\n\nfor i, sample in enumerate(balanced_test_set):\n    if len(sample['question']) > 4000:\n        continue\n    \n    prompt = TEMPLATE.format(\n        system_prompt=SYSTEM_PROMPT,\n        question=sample['question']\n    )\n    \n    try:\n        output_data = verification_sampler(\n            input_strings=[prompt],\n            max_generation_steps=MAX_TOKENS,\n            temperature=0.7,\n            echo=False,\n            eos_tokens=[1, 106],\n        )\n        output = output_data.text[0]\n        output_tokens = len(tokenizer.encode(output))\n        \n        is_valid, checks = validate_format(output)\n        \n        result = {\n            'domain': sample['domain'],\n            'question': sample['question'],\n            'valid': is_valid,\n            'output': output,\n            'output_tokens': output_tokens,\n            'checks': checks,\n            'failure_type': None,\n            'failure_reason': None,\n            'prompt': f'{str(prompt)}'\n        }\n        \n        if not is_valid:\n            failure_type, reason = analyze_format_failure(output, output_tokens, MAX_TOKENS)\n            result['failure_type'] = failure_type\n            result['failure_reason'] = reason\n            \n            if failure_type == \"TRUNCATION\":\n                truncation_failures.append(result)\n            else:\n                true_failures.append(result)\n        \n        results.append(result)\n        \n        # Progress\n        if (i + 1) % 10 == 0:\n            valid = sum(1 for r in results if r['valid'])\n            trunc = len(truncation_failures)\n            true_f = len(true_failures)\n            print(f\"Progress: {i+1}/{len(balanced_test_set)} | \"\n                  f\"Valid: {valid} | Truncated: {trunc} | True Failures: {true_f}\")\n        \n        \n    except Exception as e:\n        print(f\"âŒ Error on sample {i+1}: {e}\")\n        continue","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only_false = []\n# for i in results:\n#     if i[\"valid\"] is False:\n#         only_false.append(i)\n\n# only_false","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# RESULTS SUMMARY\n# ============================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*80)\n\ntotal = len(results)\nvalid_count = sum(1 for r in results if r['valid'])\ntrunc_count = len(truncation_failures)\ntrue_fail_count = len(true_failures)\n\n# Overall stats\nprint(f\"\\nðŸ“Š OVERALL:\")\nprint(f\"  Total tested: {total}\")\nprint(f\"  âœ… Valid format: {valid_count} ({valid_count/total*100:.1f}%)\")\nprint(f\"  âš ï¸ Truncation failures: {trunc_count} ({trunc_count/total*100:.1f}%)\")\nprint(f\"  âŒ True model failures: {true_fail_count} ({true_fail_count/total*100:.1f}%)\")\n\n# Adjusted compliance (excluding truncation)\nadjusted_total = valid_count + true_fail_count\nadjusted_rate = (valid_count / adjusted_total * 100) if adjusted_total > 0 else 0\nprint(f\"\\nðŸ“ˆ ADJUSTED (excluding truncation):\")\nprint(f\"  Format compliance: {valid_count}/{adjusted_total} ({adjusted_rate:.1f}%)\")\n\n# Per-domain breakdown\nprint(f\"\\nðŸ“‹ PER-DOMAIN BREAKDOWN:\")\nprint(f\"{'Domain':<20} {'Valid':>6} {'Trunc':>6} {'Fail':>6} {'Total':>6} {'Rate':>8}\")\nprint(\"-\" * 60)\n\ndomain_stats = defaultdict(lambda: {'valid': 0, 'trunc': 0, 'fail': 0, 'total': 0})\n\nfor r in results:\n    d = r['domain']\n    domain_stats[d]['total'] += 1\n    if r['valid']:\n        domain_stats[d]['valid'] += 1\n    elif r['failure_type'] == 'TRUNCATION':\n        domain_stats[d]['trunc'] += 1\n    else:\n        domain_stats[d]['fail'] += 1\n\nfor domain, stats in sorted(domain_stats.items()):\n    rate = (stats['valid'] / stats['total'] * 100) if stats['total'] > 0 else 0\n    adj_rate = (stats['valid'] / (stats['valid'] + stats['fail']) * 100) if (stats['valid'] + stats['fail']) > 0 else 100\n    status = \"âœ…\" if adj_rate >= 90 else \"âš ï¸\" if adj_rate >= 75 else \"âŒ\"\n    print(f\"{status} {domain:<18} {stats['valid']:>6} {stats['trunc']:>6} {stats['fail']:>6} {stats['total']:>6} {adj_rate:>7.1f}%\")\n\n# ============================================================\n# DECISION LOGIC\n# ============================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSIS & DECISION\")\nprint(\"=\"*80)\n\nif true_fail_count == 0:\n    print(\"\\nâœ… NO TRUE MODEL FAILURES!\")\n    if trunc_count > 0:\n        print(f\"   â†’ {trunc_count} truncation issues: Increase max_tokens to fix\")\n    print(\"   â†’ Model learned format correctly\")\n    print(\"   â†’ NO RETRAINING NEEDED\")\n\nelif adjusted_rate >= 90:\n    print(f\"\\nâœ… HIGH COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   â†’ {true_fail_count} true failures (acceptable)\")\n    print(f\"   â†’ {trunc_count} truncation issues (increase tokens)\")\n    print(\"   â†’ SHIP IT (competition ready)\")\n\nelif adjusted_rate >= 75:\n    print(f\"\\nâš ï¸ MODERATE COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   â†’ {true_fail_count} true failures (investigate)\")\n    print(\"   â†’ BORDERLINE: Your decision to ship or retrain\")\n\nelse:\n    print(f\"\\nâŒ LOW COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   â†’ {true_fail_count} true failures (critical)\")\n    print(\"   â†’ RETRAIN RECOMMENDED\")\n\n# Show sample failures\nif true_failures:\n    print(\"\\n\" + \"=\"*80)\n    print(\"SAMPLE TRUE FAILURES (First 3)\")\n    print(\"=\"*80)\n    \n    for i, fail in enumerate(true_failures[:3]):\n        print(f\"\\n--- Failure {i+1} ---\")\n        print(f\"Domain: {fail['domain']}\")\n        print(f\"Reason: {fail['failure_reason']}\")\n        print(f\"Tokens: {fail['output_tokens']}/{MAX_TOKENS}\")\n        print(f\"Output:\\n{fail['output']}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null}]}