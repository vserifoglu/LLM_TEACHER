{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":14291659,"sourceType":"datasetVersion","datasetId":9122651},{"sourceId":14304415,"sourceType":"datasetVersion","datasetId":9131270,"isSourceIdPinned":false}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Domain GRPO Training\n\n**Goal:** Train Gemma 3 1B with GRPO on 15K multi-domain samples\n\n**Domains:** Math, Coding, Science, Logic, Summarization, Creative Writing, Creative Ideation","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Clean up\n# !pip uninstall -q -y gensim bigframes tensorflow-decision-forests tf-keras flax jax jaxlib qwix tunix\n\n# # Install Google Cloud SDKs\n# !pip install -U -q google-cloud-storage google-cloud-automl google-cloud-bigquery protobuf\n\n# # Install NumPy 2.0\n# !pip install -q \"numpy>=2.0\" \"ml_dtypes>=0.4.0\"\n\n# # Install EXACT working versions\n# !pip install -q \\\n#     \"jax[tpu]==0.8.1\" \\\n#     \"flax==0.12.1\" \\\n#     \"qwix==0.1.4\" \\\n#     \"optax==0.2.6\" \\\n#     \"orbax-checkpoint==0.11.31\" \\\n#     \"chex==0.1.91\" \\\n#     \"google-tunix[prod]==0.1.3\" \\\n#     tensorflow \\\n#     kagglehub \\\n#     grain \\\n#     humanize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:49:58.368237Z","iopub.execute_input":"2025-12-31T00:49:58.368402Z","iopub.status.idle":"2025-12-31T00:49:58.396259Z","shell.execute_reply.started":"2025-12-31T00:49:58.368385Z","shell.execute_reply":"2025-12-31T00:49:58.395529Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nimport re\nimport csv\nimport shutil\nfrom pathlib import Path\nfrom pprint import pprint\nfrom tqdm import tqdm\n\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nfrom flax import nnx\nimport grain\nimport optax\nimport humanize\nfrom orbax import checkpoint as ocp\n\n# Tunix imports\nfrom tunix.models.gemma3 import model, params\nfrom tunix.generate import sampler as sampler_lib\n\n# GRPO-specific imports\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.rl.grpo.grpo_learner import GRPOLearner, GRPOConfig\nfrom tunix.sft import metrics_logger\n\nimport qwix\nimport numpy as np \nimport json\nimport pandas as pd\nimport re\nfrom typing import Optional, Dict, List, Tuple\nfrom abc import ABC, abstractmethod\nimport sys\nfrom typing import Dict, List\nimport pandas as pd\nimport grain\nfrom typing import Dict, Tuple, Optional, List\nfrom dataclasses import dataclass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:49:58.396709Z","iopub.execute_input":"2025-12-31T00:49:58.396876Z","iopub.status.idle":"2025-12-31T00:50:01.924298Z","shell.execute_reply.started":"2025-12-31T00:49:58.396861Z","shell.execute_reply":"2025-12-31T00:50:01.923172Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Configuration & HyperParams","metadata":{}},{"cell_type":"code","source":"# ==================== PATHS ====================\nDATASET_PATH = \"/kaggle/input/harmonic-oscillation/full_dataset_pool.jsonl\"\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/grpo_checkpoints/\"\n\n# ==================== TRAINING DATA CONFIG ====================\nTRAIN_SIZE = 15000\nVAL_SIZE = 1000\nBATCH_SIZE = 2\nDISCO_TEMP = 0.5\nSEED = 42\n\n# ==================== GRPO CONFIG ====================\nNUM_EPOCHS = 1\nNUM_ITERATIONS = 4\nNUM_GENERATIONS = 4\n\n# RL hyperparameters\nBETA = 0.04\nEPSILON = 0.2\n\n# Training config\nTRAIN_MICRO_BATCH_SIZE = 2\nMAX_STEPS = int((TRAIN_SIZE / BATCH_SIZE) * NUM_ITERATIONS * NUM_EPOCHS)\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\nMAX_GRAD_NORM = 0.1\n\n# Checkpointing\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 3\nEVAL_EVERY_N_STEPS = 500\n\n# ==================== MODEL CONFIG ====================\nMODEL_CP_PATH = params.GEMMA3_1B_IT\nMESH = ((1, 4), (\"fsdp\", \"tp\"))\n\n# LoRA config\nLORA_RANK = 16\nLORA_ALPHA = 32.0\n\n# ==================== GENERATION CONFIG ====================\nMAX_PROMPT_LENGTH = 1024          # Both need same limit\nTOTAL_GENERATION_STEPS = 512      # MUST MATCH OR INFERENCE >= TRAINING\nTEMPERATURE = 0.7\nINFERENCE_TEMPERATURE = 0.0\nTOP_P = 0.95\nTOP_K = 50\n\n# ==================== PROMPTING ====================\nSYSTEM_PROMPT = \"\"\"Provide your reasoning in <reasoning> tags, then your final answer in <answer> tags.\nFormat:\n<reasoning>Your step-by-step thinking</reasoning>\n<answer>Your final answer</answer>\"\"\"\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\nTask: {question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.924706Z","iopub.execute_input":"2025-12-31T00:50:01.925007Z","iopub.status.idle":"2025-12-31T00:50:01.929704Z","shell.execute_reply.started":"2025-12-31T00:50:01.924990Z","shell.execute_reply":"2025-12-31T00:50:01.928915Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"class DatasetLoader:\n    \"\"\"Loads and preprocesses multi-domain JSONL dataset with DISCO sampling.\"\"\"\n    \n    def __init__(self, jsonl_path: str):\n        \"\"\"\n        Initialize loader.\n        \n        Args:\n            jsonl_path: Path to JSONL file with dataset pool\n        \"\"\"\n        self.jsonl_path = jsonl_path\n        self.df = None\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load JSONL file into DataFrame.\"\"\"\n        print(f\"Loading dataset from {self.jsonl_path}...\")\n        \n        self.df = pd.read_json(self.jsonl_path, lines=True)\n        \n        # print(f\"Loaded {len(self.df)} total samples\")\n        # print(f\"Domains: {self.df['domain'].unique()}\")\n        # print(\"\\nDomain distribution:\")\n        # for domain, count in self.df['domain'].value_counts().items():\n        #     pct = (count / len(self.df)) * 100\n        #     print(f\"  {domain:20s}: {count:6d} ({pct:5.1f}%)\")\n        \n        return self.df\n    \n    def compute_disco_proportions(self, temperature: float = 1.0) -> Dict[str, float]:\n        \"\"\"\n        Compute DISCO-adjusted domain proportions.\n        \n        Args:\n            temperature: DISCO temperature\n                - T=0.5: Moderate balancin\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Get natural proportions\n        domain_counts = self.df['domain'].value_counts()\n        total = len(self.df)\n        natural_props = {domain: count / total for domain, count in domain_counts.items()}\n        \n        # Apply DISCO temperature\n        if temperature == 1.0:\n            # Natural distribution\n            adjusted = natural_props\n        else:\n            # Temperature-adjusted distribution\n            adjusted_unnormalized = {\n                domain: prop ** temperature\n                for domain, prop in natural_props.items()\n            }\n            \n            # Normalize to sum to 1.0\n            total_adjusted = sum(adjusted_unnormalized.values())\n            adjusted = {\n                domain: val / total_adjusted\n                for domain, val in adjusted_unnormalized.items()\n            }\n        \n        # ---- DEBUGGIN: Print comparison ---- \n        # print(f\"\\nDISCO Proportions (T={temperature}):\")\n        # print(f\"{'Domain':<20} {'Natural':>10} {'DISCO':>10} {'Change':>10}\")\n        # print(\"-\" * 52)\n        # for domain in sorted(natural_props.keys()):\n        #     nat = natural_props[domain]\n        #     disco = adjusted[domain]\n        #     change = ((disco - nat) / nat) * 100\n        #     print(f\"{domain:<20} {nat:>9.1%} {disco:>9.1%} {change:>+9.1f}%\")\n        \n        return adjusted\n    \n    def sample_dataset(\n        self,\n        total_size: int,\n        temperature: float,\n        seed: int\n    ) -> pd.DataFrame:\n        \"\"\"\n        Sample dataset using DISCO proportions.\n        \n        Args:\n            total_size: Total number of samples to draw\n            temperature: DISCO temperature 0.5\n            seed: Random seed for reproducibility\n        \n        Returns:\n            DataFrame with sampled data\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Compute DISCO proportions\n        proportions = self.compute_disco_proportions(temperature)\n        \n        # Sample from each domain\n        sampled_dfs = []\n        \n        print(f\"\\nSampling {total_size} samples:\")\n        for domain, proportion in proportions.items():\n            target_count = int(total_size * proportion)\n            domain_df = self.df[self.df['domain'] == domain]\n            available = len(domain_df)\n            \n            # Check if we have enough samples\n            if target_count > available:\n                sampled = domain_df\n            else:\n                sampled = domain_df.sample(n=target_count, random_state=seed)\n            \n            sampled_dfs.append(sampled)\n            print(f\"  {domain:<20}: Sampled {len(sampled):4d} samples\")\n        \n        # Combine and shuffle\n        combined = pd.concat(sampled_dfs, ignore_index=True)\n        combined = combined.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        \n        print(f\"\\nTotal: {len(combined)} samples\")\n        return combined\n    \n    def create_datasets(\n        self,\n        train_size: int,\n        temperature: float,\n        batch_size: int,\n        seed: int,\n        val_size: Optional[int] = None\n    ) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n        \"\"\"\n        Create train and validation Grain datasets.\n        \n        Args:\n            train_size: Number of training samples\n            val_size: Number of validation samples (optional)\n            temperature: DISCO temperature\n            batch_size: Batch size\n            seed: Random seed\n        \n        Returns:\n            (train_dataset, val_dataset) as Grain datasets\n        \"\"\"\n        # Sample training data\n        print(\"=\"*60)\n        print(\"TRAINING DATA\")\n        print(\"=\"*60)\n        train_df = self.sample_dataset(\n            total_size=train_size,\n            temperature=temperature,\n            seed=seed\n        )\n        \n        train_dataset = self._to_grain_dataset(train_df, batch_size, shuffle=True, seed=seed)\n        \n        # Validation dataset (optional)\n        val_dataset = None\n        if val_size:\n            print(\"\\n\" + \"=\"*60)\n            print(\"VALIDATION DATA\")\n            print(\"=\"*60)\n            val_df = self.sample_dataset(\n                total_size=val_size,\n                temperature=temperature,\n                seed=seed + 1  # Different seed\n            )\n            val_dataset = self._to_grain_dataset(val_df, batch_size, seed, shuffle=False)\n        \n        return train_dataset, val_dataset\n    \n    def _to_grain_dataset(\n        self,\n        df: pd.DataFrame,\n        batch_size: int,\n        seed: int,\n        shuffle: bool = True,\n    ) -> grain.MapDataset:\n        \"\"\"Convert DataFrame to batched Grain dataset.\"\"\"\n        # Convert to list of dicts\n        data = df.to_dict('records')\n        \n        # Create Grain dataset\n        dataset = grain.MapDataset.source(data)\n        \n        if shuffle:\n            dataset = dataset.shuffle(seed=seed)\n        \n        # Map to GRPO format (includes truncation!)\n        dataset = dataset.map(self._format_for_grpo)\n        \n        # Batch\n        dataset = dataset.batch(batch_size)\n        \n        return dataset\n    \n    def _format_for_grpo(self, item: Dict) -> Dict:\n        \"\"\"\n        Format a single item for GRPO training.\n        \n        Truncates very long prompts to fit within token limits.\n        \"\"\"\n        import json\n        \n        # Truncate long prompts\n        prompt_text = item['prompt']\n        MAX_CHARS = 2500  # ~625 tokens (safe for 1024 limit)\n        \n        if len(prompt_text) > MAX_CHARS:\n            prompt_text = prompt_text[:MAX_CHARS] + \"...\"\n        \n        # Format prompt\n        formatted_prompt = TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=prompt_text\n        )\n        \n        # Normalize metadata\n        metadata = item.get('metadata', {})\n        if not isinstance(metadata, dict):\n            metadata = {}\n        \n        return {\n            \"prompts\": formatted_prompt,\n            \"domain\": item['domain'],\n            \"question\": item['prompt'],\n            \"answer\": item['answer'],\n            \"metadata_str\": json.dumps(metadata),\n        }\n\n\n# ============================================================================\n# CONVENIENCE FUNCTIONS\n# ============================================================================\n\ndef load_dataset_for_training(\n    jsonl_path: str,\n    train_size: int,\n    batch_size: int,\n    disco_temperature: float,\n    seed: int = 42,\n    val_size: Optional[int] = None\n) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n    \"\"\"\n    Convenience function to load dataset in one call.\n    \n    Args:\n        jsonl_path: Path to JSONL dataset\n        train_size: Number of training samples\n        val_size: Number of validation samples (None to skip)\n        batch_size: Batch size\n        disco_temperature: DISCO temperature\n            - 1.0 = Natural distribution\n            - 0.5 = Moderate balancing (recommended)\n            - 0.3 = Aggressive balancing\n        seed: Random seed\n    \n    Returns:\n        (train_dataset, val_dataset)\n    \n    Example:\n        train_ds, val_ds = load_dataset_for_training(\n            '/kaggle/input/dataset/pool.jsonl',\n            train_size=15000,\n            disco_temperature=0.5\n        )\n    \"\"\"\n    loader = DatasetLoader(jsonl_path)\n    return loader.create_datasets(\n        train_size=train_size,\n        val_size=val_size,\n        temperature=disco_temperature,\n        batch_size=batch_size,\n        seed=seed\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.930506Z","iopub.execute_input":"2025-12-31T00:50:01.930684Z","iopub.status.idle":"2025-12-31T00:50:01.956142Z","shell.execute_reply.started":"2025-12-31T00:50:01.930669Z","shell.execute_reply":"2025-12-31T00:50:01.955449Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Validators","metadata":{}},{"cell_type":"code","source":"class XMLValidator:\n    \"\"\"Validates XML structure with strict ordering and uniqueness checks.\"\"\"\n    \n    REASONING_START = \"<reasoning>\"\n    REASONING_END = \"</reasoning>\"\n    ANSWER_START = \"<answer>\"\n    ANSWER_END = \"</answer>\"\n    \n    @classmethod\n    def is_valid(cls, response: str) -> bool:\n        \"\"\"\n        Check if response has valid XML structure.\n        \n        Requirements:\n        1. Exactly ONE <reasoning> tag pair\n        2. Exactly ONE <answer> tag pair\n        3. <reasoning> appears BEFORE <answer>\n        4. No overlapping/nested tags\n        5. Content not empty\n        \n        Returns:\n            bool: True if valid, False otherwise\n        \"\"\"\n        # Check tag counts\n        if response.count(cls.REASONING_START) != 1 or response.count(cls.REASONING_END) != 1:\n            return False\n        if response.count(cls.ANSWER_START) != 1 or response.count(cls.ANSWER_END) != 1:\n            return False\n        \n        # Check order\n        reasoning_start_pos = response.find(cls.REASONING_START)\n        reasoning_end_pos = response.find(cls.REASONING_END)\n        answer_start_pos = response.find(cls.ANSWER_START)\n        answer_end_pos = response.find(cls.ANSWER_END)\n        \n        # Reasoning must come before answer\n        if reasoning_start_pos >= answer_start_pos:\n            return False\n        \n        # Tags must be properly paired (start before end)\n        if reasoning_start_pos >= reasoning_end_pos:\n            return False\n        if answer_start_pos >= answer_end_pos:\n            return False\n        \n        # No overlapping (reasoning must fully end before answer starts)\n        if reasoning_end_pos >= answer_start_pos:\n            return False\n        \n        # Extract content and check not empty\n        reasoning = cls.extract_reasoning(response)\n        answer = cls.extract_answer(response)\n        \n        if not reasoning or not answer:\n            return False\n        if len(reasoning.strip()) == 0 or len(answer.strip()) == 0:\n            return False\n        \n        return True\n    \n    @classmethod\n    def extract_reasoning(cls, response: str) -> Optional[str]:\n        \"\"\"Extract reasoning content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.REASONING_START)}(.*?){re.escape(cls.REASONING_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n    \n    @classmethod\n    def extract_answer(cls, response: str) -> Optional[str]:\n        \"\"\"Extract answer content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.ANSWER_START)}(.*?){re.escape(cls.ANSWER_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-31T00:50:01.956570Z","iopub.execute_input":"2025-12-31T00:50:01.956725Z","iopub.status.idle":"2025-12-31T00:50:01.971325Z","shell.execute_reply.started":"2025-12-31T00:50:01.956710Z","shell.execute_reply":"2025-12-31T00:50:01.970444Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class DomainValidator(ABC):\n    \"\"\"Abstract base class for domain-specific answer validation.\"\"\"\n    \n    @abstractmethod\n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Check if predicted answer matches ground truth.\"\"\"\n        pass\n\n\nclass MathValidator(DomainValidator):\n    \"\"\"Validator for math domain - extracts number after ####.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Math answers are in format: \"reasoning\\n#### 60\"\n        Extract number after #### and compare.\n        \"\"\"\n        # Extract ground truth number\n        if \"####\" in ground_truth:\n            gt_value = ground_truth.split(\"####\")[1].strip()\n        else:\n            gt_value = ground_truth.strip()\n        \n        # Normalize both to float for comparison\n        try:\n            gt_num = float(self._normalize_number(gt_value))\n            pred_num = float(self._normalize_number(predicted))\n            return abs(gt_num - pred_num) < 1e-6  # Float comparison tolerance\n        except (ValueError, AttributeError):\n            return False\n    \n    @staticmethod\n    def _normalize_number(text: str) -> str:\n        \"\"\"Extract and normalize numeric value.\"\"\"\n        # Remove common formatting: commas, dollar signs, percent\n        cleaned = text.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        # Extract first number (handles cases like \"answer is 42\")\n        numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n        return numbers[0] if numbers else text\n\n\nclass CodingValidator(DomainValidator):\n    \"\"\"Validator for coding domain - executes test cases.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Execute test cases from metadata.\n        Returns True only if ALL test cases pass.\n        \"\"\"\n        if not metadata or 'test_cases' not in metadata:\n            return False\n        \n        test_cases = metadata['test_cases']\n        \n        try:\n            # Create namespace with the predicted code\n            namespace = {}\n            exec(predicted, namespace)\n            \n            # Run each test case\n            for test in test_cases:\n                try:\n                    exec(test, namespace)\n                except AssertionError:\n                    return False  # Test failed\n                except Exception:\n                    return False  # Execution error\n            \n            return True  # All tests passed\n        except Exception:\n            return False  # Code doesn't execute\n\n\nclass ScienceValidator(DomainValidator):\n    \"\"\"Validator for science domain - case-insensitive exact match.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Case-insensitive comparison after normalization.\"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        return pred_normalized == gt_normalized\n\n\nclass LogicValidator(DomainValidator):\n    \"\"\"Validator for logic domain - Yes/No normalization.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Normalize Yes/No answers.\n        Handles: yes, Yes, YES, no, No, NO\n        \"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        \n        # Check for yes/no presence\n        pred_is_yes = \"yes\" in pred_normalized\n        pred_is_no = \"no\" in pred_normalized\n        gt_is_yes = \"yes\" in gt_normalized\n        gt_is_no = \"no\" in gt_normalized\n        \n        # Match if both have same yes/no\n        return (pred_is_yes and gt_is_yes) or (pred_is_no and gt_is_no)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.971694Z","iopub.execute_input":"2025-12-31T00:50:01.971846Z","iopub.status.idle":"2025-12-31T00:50:01.986019Z","shell.execute_reply.started":"2025-12-31T00:50:01.971831Z","shell.execute_reply":"2025-12-31T00:50:01.985193Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Reward Func","metadata":{}},{"cell_type":"code","source":"class HeuristicRewards:\n    \"\"\"Heuristic-based quality rewards for creative domains.\"\"\"\n    \n    @staticmethod\n    def length_score(text: str, target: int, min_len: int, max_len: int) -> float:\n        \"\"\"\n        Score based on length appropriateness.\n        Returns 1.0 if within [min_len, max_len], decays outside.\n        \"\"\"\n        length = len(text.split())\n        \n        if min_len <= length <= max_len:\n            return 1.0\n        else:\n            # Linear decay based on distance from range\n            if length < min_len:\n                distance = min_len - length\n                max_distance = min_len\n            else:  # length > max_len\n                distance = length - max_len\n                max_distance = target\n            \n            return max(0.0, 1.0 - distance / max_distance)\n    \n    @staticmethod\n    def lexical_diversity(text: str) -> float:\n        \"\"\"\n        Calculate lexical diversity: unique words / total words.\n        \"\"\"\n        words = text.lower().split()\n        if len(words) == 0:\n            return 0.0\n        \n        unique_words = len(set(words))\n        return unique_words / len(words)\n    \n    @staticmethod\n    def prompt_relevance(prompt: str, reasoning: str) -> float:\n        \"\"\"\n        Calculate relevance by keyword overlap between prompt and reasoning.\n        \"\"\"\n        # Extract meaningful words (>3 chars, not common stop words)\n        stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', \n                     'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him',\n                     'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'way',\n                     'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}\n        \n        prompt_words = set(w.lower() for w in prompt.split() if len(w) > 3 and w.lower() not in stop_words)\n        reasoning_words = set(w.lower() for w in reasoning.split() if len(w) > 3 and w.lower() not in stop_words)\n        \n        if len(prompt_words) == 0:\n            return 0.5  # Default score if no meaningful words in prompt\n        \n        overlap = len(prompt_words & reasoning_words)\n        return overlap / len(prompt_words)\n\n\n# ============================================================================\n# MAIN REWARD CALCULATOR\n# ============================================================================\n\nclass RewardCalculator:\n    \"\"\"\n    Main reward calculator that routes to appropriate validators.\n    \n    Reward breakdown:\n    - Format: 0.2 (all domains)\n    - Verifiable: 0.6 correctness + 0.2 bonus\n    - Creative: 0.3 length + 0.25 diversity + 0.25 relevance\n    \"\"\"\n    \n    VERIFIABLE_DOMAINS = {\"math\", \"coding\", \"science\", \"logic\"}\n    CREATIVE_DOMAINS = {\"creative_writing\", \"creative_ideation\", \"summarization\"}\n    \n    def __init__(self):\n        \"\"\"Initialize validators.\"\"\"\n        self.xml_validator = XMLValidator()\n        self.validators = {\n            \"math\": MathValidator(),\n            \"coding\": CodingValidator(),\n            \"science\": ScienceValidator(),\n            \"logic\": LogicValidator(),\n        }\n        self.heuristics = HeuristicRewards()\n    \n    def compute_reward(\n        self,\n        domain: str,\n        prompt: str,\n        response: str,\n        ground_truth: Optional[str] = None,\n        metadata: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Compute total reward for a response.\n        \n        Args:\n            domain: Task domain (math, coding, science, etc.)\n            prompt: Original prompt/question\n            response: Model's generated response\n            ground_truth: Expected answer (for verifiable domains)\n            metadata: Additional data (e.g., test_cases for coding)\n        \n        Returns:\n            float: Total reward score [0.0, 1.0]\n        \"\"\"\n        # HARD DEPENDENCY: Format must be valid\n        if not self.xml_validator.is_valid(response):\n            return 0.0\n        \n        # Format is valid - start with format reward\n        reward = 0.2\n        \n        # Extract content\n        reasoning = self.xml_validator.extract_reasoning(response)\n        answer = self.xml_validator.extract_answer(response)\n        \n        # Domain-specific rewards\n        if domain in self.VERIFIABLE_DOMAINS:\n            reward += self._compute_verifiable_reward(\n                domain, answer, ground_truth, metadata\n            )\n        elif domain in self.CREATIVE_DOMAINS:\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        else:\n            # Unknown domain - use creative heuristics as fallback\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        \n        return min(reward, 1.0)  # Cap at 1.0\n    \n    def _compute_verifiable_reward(\n        self,\n        domain: str,\n        answer: str,\n        ground_truth: str,\n        metadata: Optional[Dict]\n    ) -> float:\n        \"\"\"Compute reward for verifiable domains.\"\"\"\n        validator = self.validators[domain]\n        \n        # Correctness check (0.6 points)\n        if validator.is_correct(answer, ground_truth, metadata):\n            return 0.8  # 0.6 for correctness + 0.2 bonus\n        else:\n            return 0.0\n    \n    def _compute_creative_reward(\n        self,\n        prompt: str,\n        reasoning: str,\n        answer: str\n    ) -> float:\n        \"\"\"Compute reward for creative domains using heuristics.\"\"\"\n        score = 0.0\n        \n        # Length appropriateness (0.3 points)\n        reasoning_score = self.heuristics.length_score(\n            reasoning, target=250, min_len=20, max_len=500\n        )\n        answer_score = self.heuristics.length_score(\n            answer, target=150, min_len=10, max_len=300\n        )\n        score += 0.15 * reasoning_score\n        score += 0.15 * answer_score\n        \n        # Lexical diversity (0.25 points)\n        diversity = self.heuristics.lexical_diversity(answer)\n        score += 0.25 * diversity\n        \n        # Prompt relevance (0.25 points)\n        relevance = self.heuristics.prompt_relevance(prompt, reasoning)\n        score += 0.25 * relevance\n        \n        return score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.986490Z","iopub.execute_input":"2025-12-31T00:50:01.986646Z","iopub.status.idle":"2025-12-31T00:50:02.002826Z","shell.execute_reply.started":"2025-12-31T00:50:01.986632Z","shell.execute_reply":"2025-12-31T00:50:02.002146Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_reward_batch(\n    prompts: List[str],\n    completions: List[str],\n    domain: List[str] = None,\n    answer: List[str] = None,\n    metadata_str: List[str] = None,\n    **kwargs  # Catch any other fields\n) -> List[float]:\n    \"\"\"\n    Compute rewards for a batch of responses.\n    \n    Called by GRPO with:\n    - prompts: List of prompts\n    - completions: List of model responses\n    - **kwargs: Dict with domain, answer, metadata_str, etc.\n    \n    Returns:\n        List[float]: Reward scores for each response\n    \"\"\"\n    import json\n    \n    calculator = RewardCalculator()\n    \n    # Handle missing fields\n    if domain is None:\n        domain = [\"unknown\"] * len(completions)\n    if answer is None:\n        answer = [None] * len(completions)\n    if metadata_str is None:\n        metadata_str = [\"{}\"] * len(completions)\n    \n    # Parse metadata from JSON strings\n    metadatas = []\n    for meta_str in metadata_str:\n        try:\n            metadatas.append(json.loads(meta_str) if isinstance(meta_str, str) else {})\n        except:\n            metadatas.append({})\n    \n    # Compute rewards\n    rewards = []\n    for dom, prompt, completion, gt, meta in zip(\n        domain, prompts, completions, answer, metadatas\n    ):\n        reward = calculator.compute_reward(\n            domain=dom,\n            prompt=prompt,\n            response=completion,  # GRPO calls it completion\n            ground_truth=gt,\n            metadata=meta\n        )\n        rewards.append(reward)\n    \n    return rewards","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:02.003264Z","iopub.execute_input":"2025-12-31T00:50:02.003418Z","iopub.status.idle":"2025-12-31T00:50:02.016063Z","shell.execute_reply.started":"2025-12-31T00:50:02.003403Z","shell.execute_reply":"2025-12-31T00:50:02.015381Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Memory Utility","metadata":{}},{"cell_type":"code","source":"def show_hbm_usage():\n    \"\"\"Displays memory usage per device.\"\"\"\n    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n    for d in jax.local_devices():\n        stats = d.memory_stats()\n        used = stats[\"bytes_in_use\"]\n        limit = stats[\"bytes_limit\"]\n        print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:.1%}) on {d}\")\n\nshow_hbm_usage()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-31T00:50:02.016701Z","iopub.execute_input":"2025-12-31T00:50:02.016867Z","iopub.status.idle":"2025-12-31T00:50:12.118055Z","shell.execute_reply.started":"2025-12-31T00:50:02.016852Z","shell.execute_reply":"2025-12-31T00:50:12.116903Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1767142202.072834    1496 common_lib.cc:650] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"Using 26.5 KiB / 15.7 GiB (0.0%) on TPU_0(process=0,(0,0,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_1(process=0,(1,0,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_2(process=0,(0,1,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_3(process=0,(1,1,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_4(process=0,(0,2,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_5(process=0,(1,2,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_6(process=0,(0,3,0,0))\nUsing 26.5 KiB / 15.7 GiB (0.0%) on TPU_7(process=0,(1,3,0,0))\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Cell 6: Load Dataset","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"LOADING DATASET\")\nprint(\"=\"*60)\n\ntrain_dataset, val_dataset = load_dataset_for_training(\n    jsonl_path=DATASET_PATH,\n    train_size=TRAIN_SIZE,\n    val_size=VAL_SIZE,\n    batch_size=BATCH_SIZE,\n    disco_temperature=DISCO_TEMP,\n    seed=SEED\n)\n\nprint(f\"\\n✓ Train dataset: {type(train_dataset)}\")\nprint(f\"✓ Val dataset: {type(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:12.118807Z","iopub.execute_input":"2025-12-31T00:50:12.119003Z","iopub.status.idle":"2025-12-31T00:50:13.032538Z","shell.execute_reply.started":"2025-12-31T00:50:12.118986Z","shell.execute_reply":"2025-12-31T00:50:13.031494Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING DATASET\n============================================================\n============================================================\nTRAINING DATA\n============================================================\nLoading dataset from /kaggle/input/harmonic-oscillation/full_dataset_pool.jsonl...\n\nSampling 15000 samples:\n  science             : Sampled 3415 samples\n  summarization       : Sampled 2927 samples\n  math                : Sampled 2531 samples\n  creative_writing    : Sampled 2261 samples\n  creative_ideation   : Sampled 1557 samples\n  logic               : Sampled 1391 samples\n  coding              : Sampled  913 samples\n\nTotal: 14995 samples\n\n============================================================\nVALIDATION DATA\n============================================================\n\nSampling 1000 samples:\n  science             : Sampled  227 samples\n  summarization       : Sampled  195 samples\n  math                : Sampled  168 samples\n  creative_writing    : Sampled  150 samples\n  creative_ideation   : Sampled  103 samples\n  logic               : Sampled   92 samples\n  coding              : Sampled   60 samples\n\nTotal: 995 samples\n\n✓ Train dataset: <class 'grain._src.python.dataset.transformations.batch.BatchMapDataset'>\n✓ Val dataset: <class 'grain._src.python.dataset.transformations.batch.BatchMapDataset'>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Cell 8: Load Base Model to Intermediate Checkpoint","metadata":{}},{"cell_type":"code","source":"try:\n    print(\"=\"*60)\n    print(\"LOADING BASE MODEL\")\n    print(\"=\"*60)\n    \n    # Clear any existing intermediate checkpoint\n    if os.path.exists(INTERMEDIATE_CKPT_DIR):\n        shutil.rmtree(INTERMEDIATE_CKPT_DIR)\n    os.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\n    os.makedirs(CKPT_DIR, exist_ok=True)\n    \n    # Load base Gemma model\n    config = model.ModelConfig.gemma3_1b()\n    gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n    tokenizer = params.create_tokenizer()\n    \n    # Save to intermediate checkpoint\n    checkpointer = ocp.StandardCheckpointer()\n    _, state = nnx.split(gemma)\n    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n    checkpointer.wait_until_finished()\n    \n    # Free memory\n    del gemma\n    del state\n    gc.collect()\n    \n    print(\"✓ Base model saved to intermediate checkpoint\")\n    # show_hbm_usage()\nexcept:pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:13.033171Z","iopub.execute_input":"2025-12-31T00:50:13.033361Z","iopub.status.idle":"2025-12-31T00:50:57.782865Z","shell.execute_reply.started":"2025-12-31T00:50:13.033345Z","shell.execute_reply":"2025-12-31T00:50:57.781574Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING BASE MODEL\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"ERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-3' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-4' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nIOStream.flush timed out\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\n/usr/local/lib/python3.12/site-packages/jupyter_client/session.py:835: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  for idx, buf in enumerate(buffers):\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-4' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-5' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-6' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-6' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-7' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-8' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-8' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-9' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-10' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-10' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nIOStream.flush timed out\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-15' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-16' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n/usr/local/lib/python3.12/json/decoder.py:354: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  obj, end = self.scan_once(s, idx)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-16' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-17' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-18' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-18' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-19' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-20' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-20' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-21' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-22' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-22' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-23' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-24' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-24' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-25' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-26' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-26' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-27' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-28' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-28' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-29' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-30' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-30' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-12' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-13' coro=<_async_in_context.<locals>.run_in_context() running at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-14' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n<string>:2: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-11' coro=<_async_in_context.<locals>.run_in_context() running at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-12' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-14' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\nE1231 00:50:37.928517    2615 google_auth_provider.cc:188] Could not find the credentials file in the standard gcloud location [/root/.config/gcloud/application_default_credentials.json]. You may specify a credentials file using $GOOGLE_APPLICATION_CREDENTIALS, or to use Google application default credentials, run: gcloud auth application-default login\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-70' coro=<_async_in_context.<locals>.run_in_context() running at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-2' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.task_wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n/tmp/ipykernel_1496/3690252905.py:26: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  gc.collect()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-2' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.task_wakeup()]>\n","output_type":"stream"},{"name":"stdout","text":"✓ Base model saved to intermediate checkpoint\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Cell 9: Create Reference and Policy Models","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING REFERENCE AND POLICY MODELS\")\nprint(\"=\"*60)\n\ndef get_gemma_ref_model(ckpt_path):\n    \"\"\"Load Gemma model with proper sharding.\"\"\"\n    mesh = jax.make_mesh(*MESH)\n    model_config = model.ModelConfig.gemma3_1b()\n    \n    # Create abstract model for shape inference\n    abs_gemma = nnx.eval_shape(\n        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, model_config)\n    )\n    \n    # Create sharded state specification\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    \n    # Restore checkpoint\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n    \n    # Merge graph and params\n    graph_def, _ = nnx.split(abs_gemma)\n    gemma = nnx.merge(graph_def, restored_params)\n    \n    return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n    \"\"\"Apply LoRA adapters to the model.\"\"\"\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=LORA_RANK,\n        alpha=LORA_ALPHA,\n    )\n    \n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    \n    # Apply sharding constraints\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    \n    return lora_model\n\n\n# Create reference model (frozen, for KL penalty)\nref_model, mesh, model_config = get_gemma_ref_model(\n    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n)\nprint(\"✓ Reference model loaded\")\n\n# Create policy model (will be trained with GRPO)\nlora_policy = get_lora_model(ref_model, mesh=mesh)\nprint(\"✓ Policy model with LoRA created\")\n\n# show_hbm_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:58:35.556490Z","iopub.execute_input":"2025-12-31T00:58:35.556834Z","iopub.status.idle":"2025-12-31T00:58:54.722362Z","shell.execute_reply.started":"2025-12-31T00:58:35.556813Z","shell.execute_reply":"2025-12-31T00:58:54.721151Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCREATING REFERENCE AND POLICY MODELS\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1496/2510325891.py:7: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n  mesh = jax.make_mesh(*MESH)\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n","output_type":"stream"},{"name":"stdout","text":"✓ Reference model loaded\n✓ Policy model with LoRA created\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Cell 10: Create Optimizer","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING OPTIMIZER\")\nprint(\"=\"*60)\n\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\n\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n        optimizer,\n    )\n\nprint(f\"✓ AdamW optimizer with warmup cosine decay\")\nprint(f\"  - Peak LR: {LEARNING_RATE}\")\nprint(f\"  - Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  - Grad clip norm: {MAX_GRAD_NORM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:02.030902Z","iopub.execute_input":"2025-12-31T00:59:02.031180Z","iopub.status.idle":"2025-12-31T00:59:02.036240Z","shell.execute_reply.started":"2025-12-31T00:59:02.031159Z","shell.execute_reply":"2025-12-31T00:59:02.035222Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCREATING OPTIMIZER\n============================================================\n✓ AdamW optimizer with warmup cosine decay\n  - Peak LR: 3e-06\n  - Warmup steps: 3000\n  - Grad clip norm: 0.1\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Cell 11: Configure GRPO Training","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CONFIGURING GRPO TRAINING\")\nprint(\"=\"*60)\n\n# Checkpoint saving options\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, \n    max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logging\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tensorboard/grpo\", \n    flush_every_n_steps=20\n)\n\n# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=1536,  # 1024 + 256 + 256 buffer\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1, 106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\n\nprint(\"✓ GRPO configuration complete\")\nprint(f\"  - Group size: {NUM_GENERATIONS} generations per prompt\")\nprint(f\"  - Iterations: {NUM_ITERATIONS}\")\nprint(f\"  - KL beta: {BETA}\")\nprint(f\"  - Clip epsilon: {EPSILON}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:05.693672Z","iopub.execute_input":"2025-12-31T00:59:05.693940Z","iopub.status.idle":"2025-12-31T00:59:05.699946Z","shell.execute_reply.started":"2025-12-31T00:59:05.693920Z","shell.execute_reply":"2025-12-31T00:59:05.698974Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCONFIGURING GRPO TRAINING\n============================================================\n✓ GRPO configuration complete\n  - Group size: 4 generations per prompt\n  - Iterations: 4\n  - KL beta: 0.04\n  - Clip epsilon: 0.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Cell 12: Create GRPO Trainer","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"CREATING GRPO TRAINER\")\nprint(\"=\"*60)\n\n# Create RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# Create GRPO trainer with our multi-domain reward function\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[compute_reward_batch],  # Our custom multi-domain reward!\n    grpo_config=grpo_config,\n)\n\nprint(\"✓ GRPO trainer created\")\nprint(f\"  - Actor: LoRA policy model\")\nprint(f\"  - Reference: Frozen base model\")\nprint(f\"  - Reward function: Multi-domain (7 domains)\")\n\n# show_hbm_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:09.710600Z","iopub.execute_input":"2025-12-31T00:59:09.710874Z","iopub.status.idle":"2025-12-31T00:59:11.619096Z","shell.execute_reply.started":"2025-12-31T00:59:09.710854Z","shell.execute_reply":"2025-12-31T00:59:11.618105Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"============================================================\nCREATING GRPO TRAINER\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:Reference model and actor model are colocated but do not share the same backbone. This will result in an unnecessary model copy and increased HBM usage.\n","output_type":"stream"},{"name":"stdout","text":"✓ GRPO trainer created\n  - Actor: LoRA policy model\n  - Reference: Frozen base model\n  - Reward function: Multi-domain (7 domains)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Cell 13: Run Training\n\n**This will take ~7 hours. Monitor the logs for:**\n- Average reward increasing\n- Format compliance >95%\n- No OOM errors","metadata":{}},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"STARTING GRPO TRAINING\")\nprint(\"=\"*80)\nprint(f\"Training steps: {MAX_STEPS}\")\nprint(f\"Checkpoint interval: {SAVE_INTERVAL_STEPS} steps\")\nprint(f\"Estimated time: ~7 hours\")\nprint(\"=\"*80)\n\nwith mesh:\n    grpo_trainer.train(train_dataset)\n\nprint(\"=\"*80)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:20.030827Z","iopub.execute_input":"2025-12-31T00:59:20.031070Z","execution_failed":"2025-12-31T09:45:05.277Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTARTING GRPO TRAINING\n================================================================================\nTraining steps: 30000\nCheckpoint interval: 500 steps\nEstimated time: ~7 hours\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Actor Training:   0%|          | 0/30000 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0513723aac546a181066e00d2411523"}},"metadata":{}},{"name":"stderr","text":"ERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x7f17b916c180> is already entered\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-3776' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-3777' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n/usr/local/lib/python3.12/site-packages/flax/nnx/variablelib.py:1841: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  return cls._new(children[0], dict(static))\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-3777' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Cell 14: Load Best Checkpoint","metadata":{}},{"cell_type":"code","source":"print(\"Loading latest checkpoint for evaluation...\")\n\n# Find the latest checkpoint\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n    for item in os.listdir(actor_ckpt_dir):\n        if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n            step = int(item)\n            if step > latest_step:\n                latest_step = step\n\nif latest_step == -1:\n    print(\"⚠ No checkpoints found, using current model state\")\nelse:\n    print(f\"Loading checkpoint from step {latest_step}...\")\n    \n    trained_ckpt_path = os.path.join(CKPT_DIR, \"actor\", str(latest_step), \"model_params\")\n    \n    abs_params = jax.tree.map(\n        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n        nnx.state(lora_policy, nnx.LoRAParam),\n    )\n    checkpointer = ocp.StandardCheckpointer()\n    trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n    \n    nnx.update(\n        lora_policy,\n        jax.tree.map(\n            lambda a, b: b,\n            nnx.state(lora_policy, nnx.LoRAParam),\n            trained_lora_params,\n        ),\n    )\n    print(f\"✓ Loaded checkpoint from step {latest_step}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 15: Validation","metadata":{}},{"cell_type":"markdown","source":"## Cell 16: Save Final Model","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"SAVING FINAL MODEL\")\nprint(\"=\"*60)\n\n# Save to /kaggle/working (persists after session)\nfinal_ckpt_path = \"/kaggle/working/grpo_multi_domain_final\"\n\nif os.path.exists(final_ckpt_path):\n    shutil.rmtree(final_ckpt_path)\n\n# Save LoRA parameters\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\nlora_params = nnx.state(lora_policy, nnx.LoRAParam)\ncheckpointer.save(final_ckpt_path, lora_params)\ncheckpointer.wait_until_finished()\n\nprint(f\"✓ Model saved to {final_ckpt_path}\")\n\n# Create metadata for Kaggle dataset\nimport json\nmetadata = {\n    \"title\": \"GRPO Multi-Domain Reasoning Model\",\n    \"id\": \"vserifoglu/grpo-multi-domain-final\",\n}\n\nwith open('/kaggle/working/dataset-metadata.json', 'w') as f:\n    json.dump(metadata, f)\n\nprint(\"✓ Metadata created\")\nprint(\"\\n🎉 TRAINING COMPLETE!\")\nprint(\"\\nNext steps:\")\nprint(\"1. Save notebook version\")\nprint(\"2. Go to Output tab\")\nprint(\"3. Create new dataset from output\")\nprint(\"4. Use for competition submission\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"SAVING FINAL MODEL\")\nprint(\"=\"*60)\n\n# Save to /kaggle/working (persists after session)\nfinal_ckpt_path = \"/kaggle/working/grpo_multi_domain_final\"\n\nif os.path.exists(final_ckpt_path):\n    shutil.rmtree(final_ckpt_path)\n\n# Save LoRA parameters\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\nlora_params = nnx.state(lora_policy, nnx.LoRAParam)\ncheckpointer.save(final_ckpt_path, lora_params)\ncheckpointer.wait_until_finished()\n\nprint(f\"✓ Model saved to {final_ckpt_path}\")\n\n# Upload to Kaggle Datasets (automatic!)\nimport kagglehub\n\nDATASET_HANDLE = \"fissalalsharef/grpo-multi-domain-final_v2\"\n\nprint(f\"\\nUploading to Kaggle: {DATASET_HANDLE}\")\nkagglehub.dataset_upload(\n    handle=DATASET_HANDLE,\n    local_dataset_dir=final_ckpt_path,\n    version_notes=\"Multi-domain GRPO training with DISCO balancing - Gemma 3 1B + LoRA\"\n)\n\nprint(\"✓ Dataset uploaded!\")\nprint(\"\\n🎉 TRAINING COMPLETE!\")\nprint(f\"\\nYour model is available at:\")\nprint(f\"https://www.kaggle.com/datasets/{DATASET_HANDLE}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## verify model if loadable","metadata":{}},{"cell_type":"code","source":"# print(\"=\"*80)\n# print(\"PRODUCTION VALIDATION - MODEL SETUP\")\n# print(\"=\"*80)\n\n# # Step 1: Download model from Kaggle\n# print(\"\\n1. Downloading trained model from Kaggle...\")\n# import kagglehub\n\n# DATASET_HANDLE = \"fissalalsharef/grpo-multi-domain-final\"\n# downloaded_path = kagglehub.dataset_download(DATASET_HANDLE)\n\n# print(f\"✓ Dataset downloaded to: {downloaded_path}\")\n\n# # Step 2: Load base Gemma model\n# print(\"\\n2. Loading base Gemma 3 1B model...\")\n# verification_base = params.create_model_from_checkpoint(\n#     MODEL_CP_PATH, \n#     model.ModelConfig.gemma3_1b()\n# )\n# print(\"✓ Base model loaded\")\n\n# # Step 3: Create LoRA structure\n# print(\"\\n3. Creating LoRA model structure...\")\n# verification_policy = get_lora_model(verification_base, mesh)\n\n# # Step 4: Load trained LoRA checkpoint\n# print(f\"\\n4. Loading LoRA checkpoint from Kaggle...\")\n# abs_params = jax.tree.map(\n#     lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n#     nnx.state(verification_policy, nnx.LoRAParam),\n# )\n\n# checkpointer = ocp.StandardCheckpointer()\n# loaded_params = checkpointer.restore(\n#     downloaded_path,\n#     target=abs_params\n# )\n\n# # Step 5: Apply LoRA params to base model\n# print(\"\\n5. Applying LoRA parameters...\")\n# nnx.update(\n#     verification_policy,\n#     jax.tree.map(\n#         lambda a, b: b,\n#         nnx.state(verification_policy, nnx.LoRAParam),\n#         loaded_params,\n#     ),\n# )\n# print(\"✓ LoRA applied successfully\")\n\n# # Step 6: Create sampler for testing\n# print(\"\\n6. Creating sampler...\")\n# verification_sampler = sampler_lib.Sampler(\n#     transformer=verification_policy,\n#     tokenizer=tokenizer,\n#     cache_config=sampler_lib.CacheConfig(\n#         cache_size=2048,  # 1024 + 600 for generation\n#         num_layers=model_config.num_layers,\n#         num_kv_heads=model_config.num_kv_heads,\n#         head_dim=model_config.head_dim,\n#     ),\n# )\n\n# print(\"\\n\" + \"=\"*80)\n# print(\"✅ MODEL LOADED & READY FOR VALIDATION!\")\n# print(\"=\"*80)\n# print(f\"✓ Model: {DATASET_HANDLE}\")\n# print(f\"✓ Sampler: verification_sampler\")\n# print(f\"✓ Cache size: 1624 tokens\")\n# print(\"\\n▶ Run Cell 1 to begin validation tests...\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"PRODUCTION-GRADE MODEL VALIDATION\")\nprint(\"=\"*80)\nimport random\nimport json\nfrom collections import defaultdict\n# Sample diverse test set from validation data\nprint(\"\\n1. Sampling test data...\")\ntest_samples = []\nfor batch in val_dataset:\n    for i in range(len(batch['domain'])):\n        test_samples.append({\n            'domain': batch['domain'][i],\n            'question': batch['question'][i],\n            'answer': batch['answer'][i],\n        })\n    if len(test_samples) >= 200:\n        break\n# Stratify by domain for balanced testing\ndomain_groups = defaultdict(list)\nfor sample in test_samples:\n    # if len(sample['question']) > 4000:\n    #     continue\n    domain_groups[sample['domain']].append(sample)\n# Sample evenly per domain (aim for ~15 per domain)\nbalanced_test_set = []\nsamples_per_domain = 15\nfor domain, samples in domain_groups.items():\n    sample_count = min(len(samples), samples_per_domain)\n    balanced_test_set.extend(random.sample(samples, sample_count))\nprint(f\"\\n✓ Test set created: {len(balanced_test_set)} samples\")\nprint(\"\\nDomain distribution:\")\nfor domain in set(s['domain'] for s in balanced_test_set):\n    count = sum(1 for s in balanced_test_set if s['domain'] == domain)\n    print(f\"  {domain:20s}: {count:2d} samples\")\n# Add edge cases manually\nedge_cases = [\n    {\"domain\": \"edge_short\", \"question\": \"What?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_short\", \"question\": \"?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_ambiguous\", \"question\": \"What is it?\", \"answer\": \"N/A\"},\n    {\"domain\": \"edge_ambiguous\", \"question\": \"Can you help?\", \"answer\": \"N/A\"},\n]\nprint(f\"\\n✓ Added {len(edge_cases)} edge cases\")\nprint(f\"\\nTotal test samples: {len(balanced_test_set) + len(edge_cases)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only_logic = []\n# for i in balanced_test_set:\n#     if i[\"domain\"] == \"math\":\n#         only_logic.append(i)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"TEST 1: FORMAT COMPLIANCE (with Truncation Detection)\")\nprint(\"=\"*80)\n\n# SYSTEM_PROMPT = \"\"\"You are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. \n# Then, provide the final answer between <answer> and </answer>.\"\"\"\n\n# SYSTEM_PROMPT = \"\"\"\n# You are given a query. Think about about what is required and provide your reasoning. I want you to keep track of your reasoning while figuring out the answer. \n# Once you are ready to provide the final answer, I want you to format your response as follow:\n# - for reasoning, place the entier chain of thoughts in a single tag <reasoning> and </reasoning>\n# - for the answer, place it between <answer> and </answer>\n\n# Example format:\n# <reasoning>Step by step thinking here</reasoning>\n# <answer>Final answer here</answer>\n# \"\"\"\n\n# TEMPLATE = \"\"\"<start_of_turn>user\n# {system_prompt}\n\n# query: {question}<end_of_turn>\n# <start_of_turn>model\"\"\"\n\n# SYSTEM_PROMPT = \"\"\"Task: <reasoning>...</reasoning> then <answer>...</answer>:  \n# Example format:\n# <reasoning>High level thinking here</reasoning>\n# <answer>Final answer here</answer>\n# \"\"\"\n# TEMPLATE = \"\"\"<start_of_turn>user\n# {system_prompt}\n\n# {question}<end_of_turn>\n# <start_of_turn>model\n# \"\"\"\n\n# SYSTEM_PROMPT = \"\"\"You are given a problem. Provide your step-by-step reasoning in <reasoning> tags, then your final answer in <answer> tags.\n# Format:\n# <reasoning>Your thinking process</reasoning>\n# <answer>Your final answer</answer>\"\"\"\n\n# SYSTEM_PROMPT = \"\"\"You are given a problem. Think about the problem and provide your \n#                 reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final \n#                 answer between <answer> and </answer>.\"\"\"\n# TEMPLATE = \"\"\"<start_of_turn>user\n#             {system_prompt}\n            \n#             {question}<end_of_turn>\n#             <start_of_turn>model\"\"\"\n\nMAX_TOKENS = 700  # Generous limit to reduce truncation\n\ndef analyze_format_failure(output, output_length, max_tokens):\n    \"\"\"Classify failure as truncation or true model failure\"\"\"\n    \n    has_reasoning_open = \"<reasoning>\" in output\n    has_reasoning_close = \"</reasoning>\" in output\n    has_answer_open = \"<answer>\" in output\n    has_answer_close = \"</answer>\" in output\n    \n    tags_present = sum([has_reasoning_open, has_reasoning_close, \n                        has_answer_open, has_answer_close])\n    \n    near_limit = output_length >= (max_tokens - 10)  # Within 10 tokens of limit\n    \n    # Classification logic\n    if not has_reasoning_open and not has_answer_open:\n        return \"TRUE_FAILURE\", \"Missing opening tags - model didn't learn format\"\n    \n    if near_limit and tags_present >= 2:\n        if not has_answer_close:\n            return \"TRUNCATION\", \"Hit token limit before closing </answer>\"\n        if not has_reasoning_close:\n            return \"TRUNCATION\", \"Hit token limit before closing </reasoning>\"\n    \n    if output_length < 100 and tags_present < 2:\n        return \"TRUE_FAILURE\", \"Short output with no format - model didn't try\"\n    \n    if tags_present >= 3 and not has_answer_close:\n        if near_limit:\n            return \"TRUNCATION\", \"Almost complete, hit limit\"\n        else:\n            return \"TRUE_FAILURE\", \"Had room but didn't close tags\"\n    \n    return \"TRUE_FAILURE\", \"Other format issue\"\n\n\ndef validate_format(output):\n    \"\"\"Check if format is valid\"\"\"\n    checks = {\n        'has_reasoning_open': '<reasoning>' in output,\n        'has_reasoning_close': '</reasoning>' in output,\n        'has_answer_open': '<answer>' in output,\n        'has_answer_close': '</answer>' in output,\n    }\n    \n    all_present = all(checks.values())\n    \n    if all_present:\n        r_open = output.find('<reasoning>')\n        r_close = output.find('</reasoning>')\n        a_open = output.find('<answer>')\n        a_close = output.find('</answer>')\n        \n        correct_order = (r_open < r_close < a_open < a_close)\n        checks['correct_order'] = correct_order\n        return correct_order, checks\n    \n    return False, checks\n\n\n# Run validation\nprint(f\"\\nTesting {len(balanced_test_set)} samples with {MAX_TOKENS} token limit...\")\nprint(\"(This may take 10-15 minutes)\\n\")\n\nresults = []\ntrue_failures = []\ntruncation_failures = []\n\nfor i, sample in enumerate(balanced_test_set):\n    if len(sample['question']) > 4000:\n        continue\n    \n    prompt = TEMPLATE.format(\n        system_prompt=SYSTEM_PROMPT,\n        question=sample['question']\n    )\n    \n    try:\n        output_data = verification_sampler(\n            input_strings=[prompt],\n            max_generation_steps=MAX_TOKENS,\n            temperature=0.7,\n            echo=False,\n            eos_tokens=[1, 106],\n        )\n        output = output_data.text[0]\n        output_tokens = len(tokenizer.encode(output))\n        \n        is_valid, checks = validate_format(output)\n        \n        result = {\n            'domain': sample['domain'],\n            'question': sample['question'],\n            'valid': is_valid,\n            'output': output,\n            'output_tokens': output_tokens,\n            'checks': checks,\n            'failure_type': None,\n            'failure_reason': None,\n            'prompt': f'{str(prompt)}'\n        }\n        \n        if not is_valid:\n            failure_type, reason = analyze_format_failure(output, output_tokens, MAX_TOKENS)\n            result['failure_type'] = failure_type\n            result['failure_reason'] = reason\n            \n            if failure_type == \"TRUNCATION\":\n                truncation_failures.append(result)\n            else:\n                true_failures.append(result)\n        \n        results.append(result)\n        \n        # Progress\n        if (i + 1) % 10 == 0:\n            valid = sum(1 for r in results if r['valid'])\n            trunc = len(truncation_failures)\n            true_f = len(true_failures)\n            print(f\"Progress: {i+1}/{len(balanced_test_set)} | \"\n                  f\"Valid: {valid} | Truncated: {trunc} | True Failures: {true_f}\")\n        \n        \n    except Exception as e:\n        print(f\"❌ Error on sample {i+1}: {e}\")\n        continue","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only_false = []\n# for i in results:\n#     if i[\"valid\"] is False:\n#         only_false.append(i)\n\n# only_false","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# RESULTS SUMMARY\n# ============================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*80)\n\ntotal = len(results)\nvalid_count = sum(1 for r in results if r['valid'])\ntrunc_count = len(truncation_failures)\ntrue_fail_count = len(true_failures)\n\n# Overall stats\nprint(f\"\\n📊 OVERALL:\")\nprint(f\"  Total tested: {total}\")\nprint(f\"  ✅ Valid format: {valid_count} ({valid_count/total*100:.1f}%)\")\nprint(f\"  ⚠️ Truncation failures: {trunc_count} ({trunc_count/total*100:.1f}%)\")\nprint(f\"  ❌ True model failures: {true_fail_count} ({true_fail_count/total*100:.1f}%)\")\n\n# Adjusted compliance (excluding truncation)\nadjusted_total = valid_count + true_fail_count\nadjusted_rate = (valid_count / adjusted_total * 100) if adjusted_total > 0 else 0\nprint(f\"\\n📈 ADJUSTED (excluding truncation):\")\nprint(f\"  Format compliance: {valid_count}/{adjusted_total} ({adjusted_rate:.1f}%)\")\n\n# Per-domain breakdown\nprint(f\"\\n📋 PER-DOMAIN BREAKDOWN:\")\nprint(f\"{'Domain':<20} {'Valid':>6} {'Trunc':>6} {'Fail':>6} {'Total':>6} {'Rate':>8}\")\nprint(\"-\" * 60)\n\ndomain_stats = defaultdict(lambda: {'valid': 0, 'trunc': 0, 'fail': 0, 'total': 0})\n\nfor r in results:\n    d = r['domain']\n    domain_stats[d]['total'] += 1\n    if r['valid']:\n        domain_stats[d]['valid'] += 1\n    elif r['failure_type'] == 'TRUNCATION':\n        domain_stats[d]['trunc'] += 1\n    else:\n        domain_stats[d]['fail'] += 1\n\nfor domain, stats in sorted(domain_stats.items()):\n    rate = (stats['valid'] / stats['total'] * 100) if stats['total'] > 0 else 0\n    adj_rate = (stats['valid'] / (stats['valid'] + stats['fail']) * 100) if (stats['valid'] + stats['fail']) > 0 else 100\n    status = \"✅\" if adj_rate >= 90 else \"⚠️\" if adj_rate >= 75 else \"❌\"\n    print(f\"{status} {domain:<18} {stats['valid']:>6} {stats['trunc']:>6} {stats['fail']:>6} {stats['total']:>6} {adj_rate:>7.1f}%\")\n\n# ============================================================\n# DECISION LOGIC\n# ============================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSIS & DECISION\")\nprint(\"=\"*80)\n\nif true_fail_count == 0:\n    print(\"\\n✅ NO TRUE MODEL FAILURES!\")\n    if trunc_count > 0:\n        print(f\"   → {trunc_count} truncation issues: Increase max_tokens to fix\")\n    print(\"   → Model learned format correctly\")\n    print(\"   → NO RETRAINING NEEDED\")\n\nelif adjusted_rate >= 90:\n    print(f\"\\n✅ HIGH COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   → {true_fail_count} true failures (acceptable)\")\n    print(f\"   → {trunc_count} truncation issues (increase tokens)\")\n    print(\"   → SHIP IT (competition ready)\")\n\nelif adjusted_rate >= 75:\n    print(f\"\\n⚠️ MODERATE COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   → {true_fail_count} true failures (investigate)\")\n    print(\"   → BORDERLINE: Your decision to ship or retrain\")\n\nelse:\n    print(f\"\\n❌ LOW COMPLIANCE ({adjusted_rate:.1f}%)\")\n    print(f\"   → {true_fail_count} true failures (critical)\")\n    print(\"   → RETRAIN RECOMMENDED\")\n\n# Show sample failures\nif true_failures:\n    print(\"\\n\" + \"=\"*80)\n    print(\"SAMPLE TRUE FAILURES (First 3)\")\n    print(\"=\"*80)\n    \n    for i, fail in enumerate(true_failures[:3]):\n        print(f\"\\n--- Failure {i+1} ---\")\n        print(f\"Domain: {fail['domain']}\")\n        print(f\"Reason: {fail['failure_reason']}\")\n        print(f\"Tokens: {fail['output_tokens']}/{MAX_TOKENS}\")\n        print(f\"Output:\\n{fail['output']}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}