{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14291659,"sourceType":"datasetVersion","datasetId":9122651}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Domain GRPO Training for Reasoning\n\n## Overview\n\nThis notebook demonstrates how to fine-tune **Gemma 3 1B** using **Tunix** and **GRPO (Group Relative Policy Optimization)** to teach the model structured reasoning across multiple domains.\n\n### Why This Approach?\n\n**GRPO** (inspired by [DeepSeek-R1](https://arxiv.org/abs/2401.02954)) is a reinforcement learning technique that trains models to follow specific output formats while maintaining response quality. Unlike supervised fine-tuning, GRPO learns from comparative feedback—generating multiple responses and rewarding the best ones.\n\n**Gemma 3 1B** offers an excellent balance of capability and efficiency:\n- Fits comfortably within Kaggle TPU memory constraints\n- Fast iteration cycles for experimentation\n- Strong baseline reasoning capabilities to build upon\n\n### What We Built\n\nA multi-domain reasoning model that:\n1. **Shows its work** in `<reasoning>` tags before answering\n2. **Generalizes** across 7 domains: Math, Coding, Science, Logic, Creative Writing, Summarization, and Creative Ideation\n3. **Uses DISCO sampling** to balance domain representation during training\n\n### Key Results\n\n| Domain Type | Format Compliance |\n|-------------|-------------------|\n| Verifiable (Math, Coding, Science, Logic) | **>95%** |\n| Unverifiable (Creative, Summarization) | **85-95%** |\n\n### Lessons Learned\n\n- **Prompt consistency matters**: The training prompt format must match inference exactly\n- **Token limits**: Max output tokens during training should match or exceed inference requirements\n- **Domain balance**: DISCO temperature sampling prevents overfitting to majority domains\n\n---\n\n**Domains covered:** Math • Coding • Science • Logic • Creative Writing • Summarization • Creative Ideation","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Clean up\n!pip uninstall -q -y gensim bigframes tensorflow-decision-forests tf-keras flax jax jaxlib qwix tunix\n\n# Install Google Cloud SDKs\n!pip install -U -q google-cloud-storage google-cloud-automl google-cloud-bigquery protobuf\n\n# Install NumPy 2.0\n!pip install -q \"numpy>=2.0\" \"ml_dtypes>=0.4.0\"\n\n# Install EXACT working versions\n!pip install -q \\\n    \"jax[tpu]==0.8.1\" \\\n    \"flax==0.12.1\" \\\n    \"qwix==0.1.4\" \\\n    \"optax==0.2.6\" \\\n    \"orbax-checkpoint==0.11.31\" \\\n    \"chex==0.1.91\" \\\n    \"google-tunix[prod]==0.1.3\" \\\n    tensorflow \\\n    kagglehub \\\n    grain \\\n    humanize","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Standard library\nimport functools\nimport gc\nimport json\nimport os\nimport re\nimport shutil\nimport sys\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom pprint import pprint\nfrom typing import Dict, List, Optional, Tuple\n\n# Third-party\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport numpy as np\nimport optax\nimport pandas as pd\nimport qwix\nfrom flax import nnx\nfrom orbax import checkpoint as ocp\nfrom tqdm import tqdm\n\n# Tunix\nfrom tunix.models.gemma3 import model, params\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom tunix.generate import sampler as sampler_lib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:49:58.396709Z","iopub.execute_input":"2025-12-31T00:49:58.396876Z","iopub.status.idle":"2025-12-31T00:50:01.924298Z","shell.execute_reply.started":"2025-12-31T00:49:58.396861Z","shell.execute_reply":"2025-12-31T00:50:01.923172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameters\n\n### GRPO Settings\n\n**Group Size (`NUM_GENERATIONS = 4`):** Each prompt generates 4 responses, which are compared to compute relative rewards. More generations provide better signal, but 4 is the sweet spot for 9 hours session.\n\n**Iterations (`NUM_ITERATIONS = 4`):** We update the policy 4 times per batch, extracting more learning from each data sample. It is important when training data is limited by session time.\n\n**KL Penalty (`BETA = 0.04`):** Controls how much the policy can diverge from the base model. We use a lower value to allow learning new behaviors (XML formatting) while preventing collapse.\n\n**Clip Range (`EPSILON = 0.2`):** Standard PPO clipping—prevents destructively large updates while allowing meaningful learning.\n\n---\n\n### Learning Rate & Optimization\n\n**Learning Rate (`3e-6`):** Conservative for RL. Unlike SFT, GRPO gradients can be noisy, so smaller steps improve stability.\n\n**Warmup (10% of steps):** Gradual ramp-up while early reward signals stabilize.\n\n**Gradient Clipping (`0.1`):** Tight clipping catches RL gradient spikes that could destabilize training.\n\n---\n\n### LoRA Configuration\n\n**Rank 16, Alpha 32:** Middle-ground expressiveness. Rank 16 is sufficient for learning format patterns; alpha at 2× rank is standard scaling.\n\n**Target Modules:** All attention projections (q, kv, attn_vec) plus MLP layers (gate, up, down); comprehensive coverage with ~0.5% of total parameters.\n\n---\n\n### Generation Settings\n\n**Max Prompt Length (1024):** Fits long inputs (articles, code) while leaving headroom for generation.\n\n**Generation Steps (512):** Enough for reasoning + answer. The format pattern generalizes to longer inference (700-1000 tokens).\n\n**Temperature (0.7):** Encourages diverse reasoning paths during training.","metadata":{}},{"cell_type":"code","source":"# ==================== PATHS ====================\nDATASET_PATH = \"/kaggle/input/harmonic-oscillation/full_dataset_pool.jsonl\"\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/grpo_checkpoints/\"\n\n# ==================== TRAINING DATA CONFIG ====================\nTRAIN_SIZE = 10000\nVAL_SIZE = 500\nBATCH_SIZE = 2\nDISCO_TEMP = 0.5\nSEED = 42\n\n# ==================== GRPO CONFIG ====================\nNUM_EPOCHS = 1\nNUM_ITERATIONS = 4\nNUM_GENERATIONS = 4\n\n# RL hyperparameters\nBETA = 0.04\nEPSILON = 0.2\n\n# Training config\nTRAIN_MICRO_BATCH_SIZE = 2\nMAX_STEPS = int((TRAIN_SIZE / BATCH_SIZE) * NUM_ITERATIONS * NUM_EPOCHS)\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\nMAX_GRAD_NORM = 0.1\n\n# Checkpointing\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 3\nEVAL_EVERY_N_STEPS = 500\n\n# ==================== MODEL CONFIG ====================\nMODEL_CP_PATH = params.GEMMA3_1B_IT\nMESH = ((1, 4), (\"fsdp\", \"tp\"))\n\n# LoRA config\nLORA_RANK = 16\nLORA_ALPHA = 32.0\n\n# ==================== GENERATION CONFIG ====================\nMAX_PROMPT_LENGTH = 1024          # Both need same limit\nTOTAL_GENERATION_STEPS = 512      # MUST MATCH OR INFERENCE >= TRAINING\nTEMPERATURE = 0.7\nINFERENCE_TEMPERATURE = 0.0\nTOP_P = 0.95\nTOP_K = 50\n\n# ==================== PROMPTING ====================\nSYSTEM_PROMPT = \"\"\"Provide your reasoning in <reasoning> tags, then your final answer in <answer> tags.\nFormat:\n<reasoning>Your step-by-step thinking</reasoning>\n<answer>Your final answer</answer>\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\nTask: {question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.924706Z","iopub.execute_input":"2025-12-31T00:50:01.925007Z","iopub.status.idle":"2025-12-31T00:50:01.929704Z","shell.execute_reply.started":"2025-12-31T00:50:01.924990Z","shell.execute_reply":"2025-12-31T00:50:01.928915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Training Data Prepartion\n\n### Data Sources\n\nWe curated a multi-domain dataset from 8 public sources, unified into a single JSONL file:\n\n| Domain | Source Dataset | Source | Samples |\n|--------|---------------|--------|---------|\n| Math | [GSM8K](https://www.kaggle.com/datasets/thedevastator/grade-school-math-8k-q-a) | Kaggle | 7,473 |\n| Coding | [MBPP](https://www.kaggle.com/datasets/mbppjsonl) | Kaggle | 974 |\n| Science | [SciQ](https://www.kaggle.com/datasets/thedevastator/sciq-a-dataset-for-science-question-answering) | Kaggle | 13,610 |\n| Summarization | [CNN/DailyMail](https://www.kaggle.com/datasets/newspaper-text-summarization-cnn-dailymail) | Kaggle | 10,000 |\n| Logic | [StrategyQA](https://huggingface.co/datasets/wics/strategy-qa) | HuggingFace | 2,260 |\n| Creative Writing | [WritingPrompts](https://huggingface.co/datasets/euclaise/writingprompts) | HuggingFace | 5,968 |\n| Creative Ideation | [Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) + [No Robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) | HuggingFace | 2,831 |\n\n**Total pool: 43,116 samples**\n\n### DISCO Temperature Sampling\n\nRaw datasets have imbalanced domain distributions (Science: 31.6% vs Coding: 2.3%). We use **DISCO sampling** with temperature T=0.5 to create a more balanced training set:\n\n| Domain | Natural % | DISCO T=0.5 % | Training Samples |\n|--------|-----------|---------------|------------------|\n| Science | 31.6% | 22.8% | 2,277 |\n| Summarization | 23.2% | 19.5% | 1,951 |\n| Math | 17.3% | 16.9% | 1,687 |\n| Creative Writing | 13.8% | 15.1% | 1,507 |\n| Creative Ideation | 6.6% | 10.4% | 1,038 |\n| Logic | 5.2% | 9.3% | 927 |\n| Coding | 2.3% | 6.1% | 609 |\n\nDISCO \"flattens\" the distribution, boosting underrepresented domains (Coding: 2.3% → 6.1%) while preserving some natural proportions.\n\n### Data Schema\n\nEach sample follows a unified schema:\n```json\n{\n  \"domain\": \"math\",\n  \"prompt\": \"If a train travels 60 mph for 2 hours, how far does it go?\",\n  \"answer\": \"120 miles\",\n  \"metadata\": {\"source\": \"GSM8K\"}\n}\n```\n\nFor details on data curation, see the [Dataset pipeline](https://www.kaggle.com/code/fissalalsharef/dataset-curation-pipeline)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Core Components\n\nThe following classes handle data loading, format validation, and domain-specific answer checking:\n\n- **DatasetLoader**: Loads JSONL data with DISCO sampling\n- **XMLValidator**: Ensures `<reasoning>` and `<answer>` tags are properly formatted\n- **DomainValidator**: Checks answer correctness per domain (Math, Coding, Science, Logic)","metadata":{}},{"cell_type":"code","source":"class DatasetLoader:\n    \"\"\"Loads and preprocesses multi-domain JSONL dataset with DISCO sampling.\"\"\"\n    \n    def __init__(self, jsonl_path: str):\n        \"\"\"\n        Initialize loader.\n        \n        Args:\n            jsonl_path: Path to JSONL file with dataset pool\n        \"\"\"\n        self.jsonl_path = jsonl_path\n        self.df = None\n    \n    def load(self) -> pd.DataFrame:\n        \"\"\"Load JSONL file into DataFrame.\"\"\"\n        print(f\"Loading dataset from {self.jsonl_path}...\")\n        \n        self.df = pd.read_json(self.jsonl_path, lines=True)\n        \n        return self.df\n    \n    def compute_disco_proportions(self, temperature: float = 1.0) -> Dict[str, float]:\n        \"\"\"\n        Compute DISCO-adjusted domain proportions.\n        \n        Args:\n            temperature: DISCO temperature\n                - T=0.5: Moderate balancin\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Get natural proportions\n        domain_counts = self.df['domain'].value_counts()\n        total = len(self.df)\n        natural_props = {domain: count / total for domain, count in domain_counts.items()}\n        \n        # Apply DISCO temperature\n        if temperature == 1.0:\n            # Natural distribution\n            adjusted = natural_props\n        else:\n            # Temperature-adjusted distribution\n            adjusted_unnormalized = {\n                domain: prop ** temperature\n                for domain, prop in natural_props.items()\n            }\n            \n            # Normalize to sum to 1.0\n            total_adjusted = sum(adjusted_unnormalized.values())\n            adjusted = {\n                domain: val / total_adjusted\n                for domain, val in adjusted_unnormalized.items()\n            }\n        \n        return adjusted\n    \n    def sample_dataset(\n        self,\n        total_size: int,\n        temperature: float,\n        seed: int\n    ) -> pd.DataFrame:\n        \"\"\"\n        Sample dataset using DISCO proportions.\n        \n        Args:\n            total_size: Total number of samples to draw\n            temperature: DISCO temperature 0.5\n            seed: Random seed for reproducibility\n        \n        Returns:\n            DataFrame with sampled data\n        \"\"\"\n        if self.df is None:\n            self.load()\n        \n        # Compute DISCO proportions\n        proportions = self.compute_disco_proportions(temperature)\n        \n        # Sample from each domain\n        sampled_dfs = []\n        \n        print(f\"\\nSampling {total_size} samples:\")\n        for domain, proportion in proportions.items():\n            target_count = int(total_size * proportion)\n            domain_df = self.df[self.df['domain'] == domain]\n            available = len(domain_df)\n            \n            # Check if we have enough samples\n            if target_count > available:\n                sampled = domain_df\n            else:\n                sampled = domain_df.sample(n=target_count, random_state=seed)\n            \n            sampled_dfs.append(sampled)\n            print(f\"  {domain:<20}: Sampled {len(sampled):4d} samples\")\n        \n        # Combine and shuffle\n        combined = pd.concat(sampled_dfs, ignore_index=True)\n        combined = combined.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        \n        print(f\"\\nTotal: {len(combined)} samples\")\n        return combined\n    \n    def create_datasets(\n        self,\n        train_size: int,\n        temperature: float,\n        batch_size: int,\n        seed: int,\n        val_size: Optional[int] = None\n    ) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n        \"\"\"\n        Create train and validation Grain datasets.\n        \n        Args:\n            train_size: Number of training samples\n            val_size: Number of validation samples (optional)\n            temperature: DISCO temperature\n            batch_size: Batch size\n            seed: Random seed\n        \n        Returns:\n            (train_dataset, val_dataset) as Grain datasets\n        \"\"\"\n        # Sample training data\n        print(\"=\"*60)\n        print(\"TRAINING DATA\")\n        print(\"=\"*60)\n        train_df = self.sample_dataset(\n            total_size=train_size,\n            temperature=temperature,\n            seed=seed\n        )\n        \n        train_dataset = self._to_grain_dataset(train_df, batch_size, shuffle=True, seed=seed)\n        \n        # Validation dataset (optional)\n        val_dataset = None\n        if val_size:\n            print(\"\\n\" + \"=\"*60)\n            print(\"VALIDATION DATA\")\n            print(\"=\"*60)\n            val_df = self.sample_dataset(\n                total_size=val_size,\n                temperature=temperature,\n                seed=seed + 1  # Different seed\n            )\n            val_dataset = self._to_grain_dataset(val_df, batch_size, seed, shuffle=False)\n        \n        return train_dataset, val_dataset\n    \n    def _to_grain_dataset(\n        self,\n        df: pd.DataFrame,\n        batch_size: int,\n        seed: int,\n        shuffle: bool = True,\n    ) -> grain.MapDataset:\n        \"\"\"Convert DataFrame to batched Grain dataset.\"\"\"\n        # Convert to list of dicts\n        data = df.to_dict('records')\n        \n        # Create Grain dataset\n        dataset = grain.MapDataset.source(data)\n        \n        if shuffle:\n            dataset = dataset.shuffle(seed=seed)\n        \n        # Map to GRPO format (includes truncation!)\n        dataset = dataset.map(self._format_for_grpo)\n        \n        # Batch\n        dataset = dataset.batch(batch_size)\n        \n        return dataset\n    \n    def _format_for_grpo(self, item: Dict) -> Dict:\n        \"\"\"\n        Format a single item for GRPO training.\n        \n        Truncates very long prompts to fit within token limits.\n        \"\"\"\n        \n        # Truncate long prompts\n        prompt_text = item['prompt']\n        MAX_CHARS = 2500  # ~625 tokens (safe for 1024 limit)\n        \n        if len(prompt_text) > MAX_CHARS:\n            prompt_text = prompt_text[:MAX_CHARS] + \"...\"\n        \n        # Format prompt\n        formatted_prompt = TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=prompt_text\n        )\n        \n        # Normalize metadata\n        metadata = item.get('metadata', {})\n        if not isinstance(metadata, dict):\n            metadata = {}\n        \n        return {\n            \"prompts\": formatted_prompt,\n            \"domain\": item['domain'],\n            \"question\": item['prompt'],\n            \"answer\": item['answer'],\n            \"metadata_str\": json.dumps(metadata),\n        }\n\ndef load_dataset_for_training(\n    jsonl_path: str,\n    train_size: int,\n    batch_size: int,\n    disco_temperature: float,\n    seed: int = 42,\n    val_size: Optional[int] = None\n) -> Tuple[grain.MapDataset, Optional[grain.MapDataset]]:\n    \"\"\"\n    Convenience function to load dataset in one call.\n    \n    Args:\n        jsonl_path: Path to JSONL dataset\n        train_size: Number of training samples\n        val_size: Number of validation samples (None to skip)\n        batch_size: Batch size\n        disco_temperature: DISCO temperature\n            - 0.5 = Moderate balancing\n        seed: Random seed\n    \n    Returns:\n        (train_dataset, val_dataset)\n    \n    Example:\n        train_ds, val_ds = load_dataset_for_training(\n            '/kaggle/input/dataset/pool.jsonl',\n            train_size=15000,\n            disco_temperature=0.5\n        )\n    \"\"\"\n    loader = DatasetLoader(jsonl_path)\n    return loader.create_datasets(\n        train_size=train_size,\n        val_size=val_size,\n        temperature=disco_temperature,\n        batch_size=batch_size,\n        seed=seed\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.930506Z","iopub.execute_input":"2025-12-31T00:50:01.930684Z","iopub.status.idle":"2025-12-31T00:50:01.956142Z","shell.execute_reply.started":"2025-12-31T00:50:01.930669Z","shell.execute_reply":"2025-12-31T00:50:01.955449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Training & Validation dataset.\nprint(\"=\"*60)\nprint(\"LOADING DATASET\")\nprint(\"=\"*60)\n\ntrain_dataset, val_dataset = load_dataset_for_training(\n    jsonl_path=DATASET_PATH,\n    train_size=TRAIN_SIZE,\n    val_size=VAL_SIZE,\n    batch_size=BATCH_SIZE,\n    disco_temperature=DISCO_TEMP,\n    seed=SEED\n)\n\nprint(f\"\\n✓ Train dataset: {type(train_dataset)}\")\nprint(f\"✓ Val dataset: {type(val_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class XMLValidator:\n    \"\"\"Validates XML structure with strict ordering and uniqueness checks.\"\"\"\n    \n    REASONING_START = \"<reasoning>\"\n    REASONING_END = \"</reasoning>\"\n    ANSWER_START = \"<answer>\"\n    ANSWER_END = \"</answer>\"\n    \n    @classmethod\n    def is_valid(cls, response: str) -> bool:\n        \"\"\"\n        Check if response has valid XML structure.\n        \n        Requirements:\n        1. Exactly ONE <reasoning> tag pair\n        2. Exactly ONE <answer> tag pair\n        3. <reasoning> appears BEFORE <answer>\n        4. No overlapping/nested tags\n        5. Content not empty\n        \n        Returns:\n            bool: True if valid, False otherwise\n        \"\"\"\n        # Check tag counts\n        if response.count(cls.REASONING_START) != 1 or response.count(cls.REASONING_END) != 1:\n            return False\n        if response.count(cls.ANSWER_START) != 1 or response.count(cls.ANSWER_END) != 1:\n            return False\n        \n        # Check order\n        reasoning_start_pos = response.find(cls.REASONING_START)\n        reasoning_end_pos = response.find(cls.REASONING_END)\n        answer_start_pos = response.find(cls.ANSWER_START)\n        answer_end_pos = response.find(cls.ANSWER_END)\n        \n        # Reasoning must come before answer\n        if reasoning_start_pos >= answer_start_pos:\n            return False\n        \n        # Tags must be properly paired (start before end)\n        if reasoning_start_pos >= reasoning_end_pos:\n            return False\n        if answer_start_pos >= answer_end_pos:\n            return False\n        \n        # No overlapping (reasoning must fully end before answer starts)\n        if reasoning_end_pos >= answer_start_pos:\n            return False\n        \n        # Extract content and check not empty\n        reasoning = cls.extract_reasoning(response)\n        answer = cls.extract_answer(response)\n        \n        if not reasoning or not answer:\n            return False\n        if len(reasoning.strip()) == 0 or len(answer.strip()) == 0:\n            return False\n        \n        return True\n    \n    @classmethod\n    def extract_reasoning(cls, response: str) -> Optional[str]:\n        \"\"\"Extract reasoning content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.REASONING_START)}(.*?){re.escape(cls.REASONING_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n    \n    @classmethod\n    def extract_answer(cls, response: str) -> Optional[str]:\n        \"\"\"Extract answer content between tags.\"\"\"\n        pattern = rf\"{re.escape(cls.ANSWER_START)}(.*?){re.escape(cls.ANSWER_END)}\"\n        match = re.search(pattern, response, re.DOTALL)\n        return match.group(1).strip() if match else None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.956570Z","iopub.execute_input":"2025-12-31T00:50:01.956725Z","iopub.status.idle":"2025-12-31T00:50:01.971325Z","shell.execute_reply.started":"2025-12-31T00:50:01.956710Z","shell.execute_reply":"2025-12-31T00:50:01.970444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DomainValidator(ABC):\n    \"\"\"Abstract base class for domain-specific answer validation.\"\"\"\n    \n    @abstractmethod\n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Check if predicted answer matches ground truth.\"\"\"\n        pass\n\n\nclass MathValidator(DomainValidator):\n    \"\"\"Validator for math domain - extracts number after ####.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Math answers are in format: \"reasoning\\n#### 60\"\n        Extract number after #### and compare.\n        \"\"\"\n        # Extract ground truth number\n        if \"####\" in ground_truth:\n            gt_value = ground_truth.split(\"####\")[1].strip()\n        else:\n            gt_value = ground_truth.strip()\n        \n        # Normalize both to float for comparison\n        try:\n            gt_num = float(self._normalize_number(gt_value))\n            pred_num = float(self._normalize_number(predicted))\n            return abs(gt_num - pred_num) < 1e-6  # Float comparison tolerance\n        except (ValueError, AttributeError):\n            return False\n    \n    @staticmethod\n    def _normalize_number(text: str) -> str:\n        \"\"\"Extract and normalize numeric value.\"\"\"\n        # Remove common formatting: commas, dollar signs, percent\n        cleaned = text.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        # Extract first number (handles cases like \"answer is 42\")\n        numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n        return numbers[0] if numbers else text\n\n\nclass CodingValidator(DomainValidator):\n    \"\"\"Validator for coding domain - executes test cases.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Execute test cases from metadata.\n        Returns True only if ALL test cases pass.\n        \"\"\"\n        if not metadata or 'test_cases' not in metadata:\n            return False\n        \n        test_cases = metadata['test_cases']\n        \n        try:\n            # Create namespace with the predicted code\n            namespace = {}\n            exec(predicted, namespace)\n            \n            # Run each test case\n            for test in test_cases:\n                try:\n                    exec(test, namespace)\n                except AssertionError:\n                    return False  # Test failed\n                except Exception:\n                    return False  # Execution error\n            \n            return True  # All tests passed\n        except Exception:\n            return False  # Code doesn't execute\n\n\nclass ScienceValidator(DomainValidator):\n    \"\"\"Validator for science domain - case-insensitive exact match.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"Case-insensitive comparison after normalization.\"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        return pred_normalized == gt_normalized\n\n\nclass LogicValidator(DomainValidator):\n    \"\"\"Validator for logic domain - Yes/No normalization.\"\"\"\n    \n    def is_correct(self, predicted: str, ground_truth: str, metadata: Dict = None) -> bool:\n        \"\"\"\n        Normalize Yes/No answers.\n        Handles: yes, Yes, YES, no, No, NO\n        \"\"\"\n        pred_normalized = predicted.strip().lower()\n        gt_normalized = ground_truth.strip().lower()\n        \n        # Check for yes/no presence\n        pred_is_yes = \"yes\" in pred_normalized\n        pred_is_no = \"no\" in pred_normalized\n        gt_is_yes = \"yes\" in gt_normalized\n        gt_is_no = \"no\" in gt_normalized\n        \n        # Match if both have same yes/no\n        return (pred_is_yes and gt_is_yes) or (pred_is_no and gt_is_no)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.971694Z","iopub.execute_input":"2025-12-31T00:50:01.971846Z","iopub.status.idle":"2025-12-31T00:50:01.986019Z","shell.execute_reply.started":"2025-12-31T00:50:01.971831Z","shell.execute_reply":"2025-12-31T00:50:01.985193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Reward Function Design\n\nGRPO learns from reward signals. Our reward function has two key properties:\n\n1. **Format compliance is mandatory** — invalid XML = 0 reward\n2. **Domain-specific scoring** — verifiable domains check correctness, creative domains use heuristics\n\n### Reward Structure\n\n| Component | Points | Condition |\n|-----------|--------|-----------|\n| **Format (Hard Gate)** | 0.2 | Valid `<reasoning>` and `<answer>` tags |\n| **Correctness** (verifiable) | 0.8 | Answer matches ground truth |\n| **Quality** (unverifiable) | up to 0.8 | Length + Diversity + Relevance |\n\n**Total: 0.0 to 1.0**\n\n### Format Validation Rules\n\nThe XML validator enforces strict requirements:\n- Exactly ONE `<reasoning>` tag pair\n- Exactly ONE `<answer>` tag pair  \n- `<reasoning>` must appear BEFORE `<answer>`\n- No overlapping or nested tags\n- Both sections must have non-empty content\n\n**Why a hard gate?** If output isn't parseable, we can't evaluate it. Reward = 0 teaches the model: \"format first, content second.\"\n\n### Verifiable Domain Validators\n\n| Domain | Validation Method |\n|--------|-------------------|\n| **Math** | Extract number after `####`, compare with tolerance |\n| **Coding** | Execute code against test cases |\n| **Science** | Case-insensitive exact match |\n| **Logic** | Normalize Yes/No answers |\n\n### Unverifiable Domain Heuristics\n\nFor creative domains (writing, ideation, summarization), we use quality heuristics:\n\n| Heuristic | Weight | What It Measures |\n|-----------|--------|------------------|\n| **Length Score** | 0.30 | Reasoning: 20-500 words, Answer: 10-300 words |\n| **Lexical Diversity** | 0.25 | Unique words / Total words (vocabulary richness) |\n| **Prompt Relevance** | 0.25 | Keyword overlap between prompt and reasoning |\n\n### Example Reward Calculations\n\n**Math problem (correct answer):**\nFormat valid: +0.2 Answer correct: +0.8 Total: 1.0 ✓\n\n**Creative writing (good response):**\nFormat valid: +0.2 Length appropriate: +0.25 High diversity: +0.20 Good relevance: +0.20 Total: 0.85 ✓\n\n**Any domain (invalid format):**\nFormat invalid: 0.0 Total: 0.0 ✗\n","metadata":{}},{"cell_type":"code","source":"class HeuristicRewards:\n    \"\"\"Heuristic-based quality rewards for creative domains.\"\"\"\n    \n    @staticmethod\n    def length_score(text: str, target: int, min_len: int, max_len: int) -> float:\n        \"\"\"\n        Score based on length appropriateness.\n        Returns 1.0 if within [min_len, max_len], decays outside.\n        \"\"\"\n        length = len(text.split())\n        \n        if min_len <= length <= max_len:\n            return 1.0\n        else:\n            # Linear decay based on distance from range\n            if length < min_len:\n                distance = min_len - length\n                max_distance = min_len\n            else:  # length > max_len\n                distance = length - max_len\n                max_distance = target\n            \n            return max(0.0, 1.0 - distance / max_distance)\n    \n    @staticmethod\n    def lexical_diversity(text: str) -> float:\n        \"\"\"\n        Calculate lexical diversity: unique words / total words.\n        \"\"\"\n        words = text.lower().split()\n        if len(words) == 0:\n            return 0.0\n        \n        unique_words = len(set(words))\n        return unique_words / len(words)\n    \n    @staticmethod\n    def prompt_relevance(prompt: str, reasoning: str) -> float:\n        \"\"\"\n        Calculate relevance by keyword overlap between prompt and reasoning.\n        \"\"\"\n        # Extract meaningful words (>3 chars, not common stop words)\n        stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', \n                     'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him',\n                     'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'way',\n                     'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}\n        \n        prompt_words = set(w.lower() for w in prompt.split() if len(w) > 3 and w.lower() not in stop_words)\n        reasoning_words = set(w.lower() for w in reasoning.split() if len(w) > 3 and w.lower() not in stop_words)\n        \n        if len(prompt_words) == 0:\n            return 0.5  # Default score if no meaningful words in prompt\n        \n        overlap = len(prompt_words & reasoning_words)\n        return overlap / len(prompt_words)\n\n\n# ============================================================================\n# MAIN REWARD CALCULATOR\n# ============================================================================\nclass RewardCalculator:\n    \"\"\"\n    Main reward calculator that routes to appropriate validators.\n    \n    Reward breakdown:\n    - Format: 0.2 (all domains)\n    - Verifiable: 0.6 correctness + 0.2 bonus\n    - Creative: 0.3 length + 0.25 diversity + 0.25 relevance\n    \"\"\"\n    \n    VERIFIABLE_DOMAINS = {\"math\", \"coding\", \"science\", \"logic\"}\n    CREATIVE_DOMAINS = {\"creative_writing\", \"creative_ideation\", \"summarization\"}\n    \n    def __init__(self):\n        \"\"\"Initialize validators.\"\"\"\n        self.xml_validator = XMLValidator()\n        self.validators = {\n            \"math\": MathValidator(),\n            \"coding\": CodingValidator(),\n            \"science\": ScienceValidator(),\n            \"logic\": LogicValidator(),\n        }\n        self.heuristics = HeuristicRewards()\n    \n    def compute_reward(\n        self,\n        domain: str,\n        prompt: str,\n        response: str,\n        ground_truth: Optional[str] = None,\n        metadata: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Compute total reward for a response.\n        \n        Args:\n            domain: Task domain (math, coding, science, etc.)\n            prompt: Original prompt/question\n            response: Model's generated response\n            ground_truth: Expected answer (for verifiable domains)\n            metadata: Additional data (e.g., test_cases for coding)\n        \n        Returns:\n            float: Total reward score [0.0, 1.0]\n        \"\"\"\n        # HARD DEPENDENCY: Format must be valid\n        if not self.xml_validator.is_valid(response):\n            return 0.0\n        \n        # Format is valid - start with format reward\n        reward = 0.2\n        \n        # Extract content\n        reasoning = self.xml_validator.extract_reasoning(response)\n        answer = self.xml_validator.extract_answer(response)\n        \n        # Domain-specific rewards\n        if domain in self.VERIFIABLE_DOMAINS:\n            reward += self._compute_verifiable_reward(\n                domain, answer, ground_truth, metadata\n            )\n        elif domain in self.CREATIVE_DOMAINS:\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        else:\n            # Unknown domain - use creative heuristics as fallback\n            reward += self._compute_creative_reward(\n                prompt, reasoning, answer\n            )\n        \n        return min(reward, 1.0)  # Cap at 1.0\n    \n    def _compute_verifiable_reward(\n        self,\n        domain: str,\n        answer: str,\n        ground_truth: str,\n        metadata: Optional[Dict]\n    ) -> float:\n        \"\"\"Compute reward for verifiable domains.\"\"\"\n        validator = self.validators[domain]\n        \n        # Correctness check (0.6 points)\n        if validator.is_correct(answer, ground_truth, metadata):\n            return 0.8  # 0.6 for correctness + 0.2 bonus\n        else:\n            return 0.0\n    \n    def _compute_creative_reward(\n        self,\n        prompt: str,\n        reasoning: str,\n        answer: str\n    ) -> float:\n        \"\"\"Compute reward for creative domains using heuristics.\"\"\"\n        score = 0.0\n        \n        # Length appropriateness (0.3 points)\n        reasoning_score = self.heuristics.length_score(\n            reasoning, target=250, min_len=20, max_len=500\n        )\n        answer_score = self.heuristics.length_score(\n            answer, target=150, min_len=10, max_len=300\n        )\n        score += 0.15 * reasoning_score\n        score += 0.15 * answer_score\n        \n        # Lexical diversity (0.25 points)\n        diversity = self.heuristics.lexical_diversity(answer)\n        score += 0.25 * diversity\n        \n        # Prompt relevance (0.25 points)\n        relevance = self.heuristics.prompt_relevance(prompt, reasoning)\n        score += 0.25 * relevance\n        \n        return score\n\ndef compute_reward_batch(\n    prompts: List[str],\n    completions: List[str],\n    domain: List[str] = None,\n    answer: List[str] = None,\n    metadata_str: List[str] = None,\n    **kwargs  # Catch any other fields\n) -> List[float]:\n    \"\"\"\n    Compute rewards for a batch of responses.\n    \n    Called by GRPO with:\n    - prompts: List of prompts\n    - completions: List of model responses\n    - **kwargs: Dict with domain, answer, metadata_str, etc.\n    \n    Returns:\n        List[float]: Reward scores for each response\n    \"\"\"    \n    calculator = RewardCalculator()\n    \n    # Handle missing fields\n    if domain is None:\n        domain = [\"unknown\"] * len(completions)\n    if answer is None:\n        answer = [None] * len(completions)\n    if metadata_str is None:\n        metadata_str = [\"{}\"] * len(completions)\n    \n    # Parse metadata from JSON strings\n    metadatas = []\n    for meta_str in metadata_str:\n        try:\n            metadatas.append(json.loads(meta_str) if isinstance(meta_str, str) else {})\n        except:\n            metadatas.append({})\n    \n    # Compute rewards\n    rewards = []\n    for dom, prompt, completion, gt, meta in zip(\n        domain, prompts, completions, answer, metadatas\n    ):\n        reward = calculator.compute_reward(\n            domain=dom,\n            prompt=prompt,\n            response=completion,  # GRPO calls it completion\n            ground_truth=gt,\n            metadata=meta\n        )\n        rewards.append(reward)\n    \n    return rewards\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:01.986490Z","iopub.execute_input":"2025-12-31T00:50:01.986646Z","iopub.status.idle":"2025-12-31T00:50:02.002826Z","shell.execute_reply.started":"2025-12-31T00:50:01.986632Z","shell.execute_reply":"2025-12-31T00:50:02.002146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Training Pipeline\n\nThe following cells set up and run the GRPO training loop:\n\n1. **Load Base Model** → Save to intermediate checkpoint for reference model\n2. **Create Models** → Reference (frozen) + Policy (LoRA-wrapped)\n3. **Create Optimizer** → AdamW with warmup cosine schedule\n4. **Configure GRPO** → Rollout config, checkpointing, metrics logging\n5. **Create Trainer** → Wire up models, tokenizer, and reward function\n6. **Run Training** → Execute the GRPO loop\n7. **Save Checkpoint** → Saving final checkpoint.\n\n**Note:** Training takes ~7-9 hours on Kaggle TPU v5-8. Checkpoints are saved every 500 steps to `/tmp/content/grpo_checkpoints/`.","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"1. LOADING BASE MODEL\")\nprint(\"=\"*60)\n\n# Clear any existing intermediate checkpoint\nif os.path.exists(INTERMEDIATE_CKPT_DIR):\n    shutil.rmtree(INTERMEDIATE_CKPT_DIR)\nos.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\nos.makedirs(CKPT_DIR, exist_ok=True)\n\n# Load base Gemma model\nconfig = model.ModelConfig.gemma3_1b()\ngemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\ntokenizer = params.create_tokenizer()\n\n# Save to intermediate checkpoint\ncheckpointer = ocp.StandardCheckpointer()\n_, state = nnx.split(gemma)\ncheckpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\ncheckpointer.wait_until_finished()\n\n# Free memory\ndel gemma\ndel state\ngc.collect()\n\nprint(\"✓ Base model saved to intermediate checkpoint\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:13.033171Z","iopub.execute_input":"2025-12-31T00:50:13.033361Z","iopub.status.idle":"2025-12-31T00:50:57.782865Z","shell.execute_reply.started":"2025-12-31T00:50:13.033345Z","shell.execute_reply":"2025-12-31T00:50:57.781574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"2. CREATING REFERENCE AND POLICY MODELS\")\nprint(\"=\"*60)\n\ndef get_gemma_ref_model(ckpt_path):\n    \"\"\"Load Gemma model with proper sharding.\"\"\"\n    mesh = jax.make_mesh(*MESH)\n    model_config = model.ModelConfig.gemma3_1b()\n    \n    # Create abstract model for shape inference\n    abs_gemma = nnx.eval_shape(\n        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, model_config)\n    )\n    \n    # Create sharded state specification\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    \n    # Restore checkpoint\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n    \n    # Merge graph and params\n    graph_def, _ = nnx.split(abs_gemma)\n    gemma = nnx.merge(graph_def, restored_params)\n    \n    return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n    \"\"\"Apply LoRA adapters to the model.\"\"\"\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=LORA_RANK,\n        alpha=LORA_ALPHA,\n    )\n    \n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    \n    # Apply sharding constraints\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    \n    return lora_model\n\n\n# Create reference model (frozen, for KL penalty)\nref_model, mesh, model_config = get_gemma_ref_model(\n    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n)\nprint(\"✓ Reference model loaded\")\n\n# Create policy model (will be trained with GRPO)\nlora_policy = get_lora_model(ref_model, mesh=mesh)\nprint(\"✓ Policy model with LoRA created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:58:35.556490Z","iopub.execute_input":"2025-12-31T00:58:35.556834Z","iopub.status.idle":"2025-12-31T00:58:54.722362Z","shell.execute_reply.started":"2025-12-31T00:58:35.556813Z","shell.execute_reply":"2025-12-31T00:58:54.721151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"3. CREATING OPTIMIZER\")\nprint(\"=\"*60)\n\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\n\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n        optimizer,\n    )\n\nprint(f\"AdamW optimizer with warmup cosine decay\")\nprint(f\"  - Peak LR: {LEARNING_RATE}\")\nprint(f\"  - Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  - Grad clip norm: {MAX_GRAD_NORM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:02.030902Z","iopub.execute_input":"2025-12-31T00:59:02.031180Z","iopub.status.idle":"2025-12-31T00:59:02.036240Z","shell.execute_reply.started":"2025-12-31T00:59:02.031159Z","shell.execute_reply":"2025-12-31T00:59:02.035222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"4. CONFIGURING GRPO TRAINING\")\nprint(\"=\"*60)\n\n# Checkpoint saving options\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, \n    max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logging\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tensorboard/grpo\", \n    flush_every_n_steps=20\n)\n\n# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=1536,  # 1024 + 256 + 256 buffer\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1, 106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\n\nprint(\"✓ GRPO configuration complete\")\nprint(f\"  - Group size: {NUM_GENERATIONS} generations per prompt\")\nprint(f\"  - Iterations: {NUM_ITERATIONS}\")\nprint(f\"  - KL beta: {BETA}\")\nprint(f\"  - Clip epsilon: {EPSILON}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:05.693672Z","iopub.execute_input":"2025-12-31T00:59:05.693940Z","iopub.status.idle":"2025-12-31T00:59:05.699946Z","shell.execute_reply.started":"2025-12-31T00:59:05.693920Z","shell.execute_reply":"2025-12-31T00:59:05.698974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"5. CREATING GRPO TRAINER\")\nprint(\"=\"*60)\n\n# Create RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# Create GRPO trainer with our multi-domain reward function\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[compute_reward_batch],  # Our custom multi-domain reward!\n    grpo_config=grpo_config,\n)\n\nprint(\"✓ GRPO trainer created\")\nprint(f\"  - Actor: LoRA policy model\")\nprint(f\"  - Reference: Frozen base model\")\nprint(f\"  - Reward function: Multi-domain (7 domains)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:09.710600Z","iopub.execute_input":"2025-12-31T00:59:09.710874Z","iopub.status.idle":"2025-12-31T00:59:11.619096Z","shell.execute_reply.started":"2025-12-31T00:59:09.710854Z","shell.execute_reply":"2025-12-31T00:59:11.618105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"6. STARTING GRPO TRAINING\")\nprint(\"=\"*80)\nprint(f\"Training steps: {MAX_STEPS}\")\nprint(f\"Checkpoint interval: {SAVE_INTERVAL_STEPS} steps\")\nprint(\"=\"*80)\n\nwith mesh:\n    grpo_trainer.train(train_dataset)\n\nprint(\"=\"*80)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:59:20.030827Z","iopub.execute_input":"2025-12-31T00:59:20.031070Z","execution_failed":"2025-12-31T09:45:05.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOTE: This saves LoRA parameters only.\n# To use: Load base Gemma 3 1B → Apply LoRA → Restore these params.\n\nprint(\"=\"*60)\nprint(\"7. SAVING FINAL checkpoint\")\nprint(\"=\"*60)\n\n# Save to /kaggle/working (persists after session)\nfinal_ckpt_path = \"/kaggle/working/grpo_multi_domain_final\"\n\nif os.path.exists(final_ckpt_path):\n    shutil.rmtree(final_ckpt_path)\n\n# Save LoRA parameters\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\nlora_params = nnx.state(lora_policy, nnx.LoRAParam)\ncheckpointer.save(final_ckpt_path, lora_params)\ncheckpointer.wait_until_finished()\n\nprint(f\"✓ Model saved to {final_ckpt_path}\")\n\n# Create metadata for Kaggle dataset\nmetadata = {\n    \"title\": \"GRPO Multi-Domain Reasoning Model\",\n    \"id\": \"fissalalsharef/grpo-multi-domain-final\",\n}\n\nwith open('/kaggle/working/dataset-metadata.json', 'w') as f:\n    json.dump(metadata, f)\n\nprint(\"✓ Metadata created\")\nprint(\"\\nTRAINING COMPLETE!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-31T09:45:05.273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Sample Results\n\n**Note:** Inference is not run in this notebook to preserve the 9-hour session limit. \nResults below are from separate validation runs on held-out test data.\n\nDatasets used for validation:\n\n- [Uverifiable Domains](https://www.kaggle.com/code/fissalalsharef/stress-testing-unverifiable-domain-dataset)\n- [Verifiable & Unverifiable Domains](https://www.kaggle.com/code/fissalalsharef/stress-testing-verifiable-domain-dataset)\n\n### Format Compliance by Domain\n\n**Verifiable Domains:**\n| Domain | Accuracy |\n|--------|----------|\n| Math | **93.9%** |\n| Coding | **93.9%** |\n\n**Unverifiable Domains:**\n| Domain | Valid Rate |\n|--------|------------|\n| Summarization | **95.0%** |\n| Conversation | 87.9% |\n| Creative Writing | 82.1% |\n\n### LLM-as-a-Judge Scores (DeepSeek Reasoner)\n\n| Domain | Reasoning | Answer | Overall |\n|--------|-----------|--------|---------|\n| Creative Writing | 4.00/5 | 3.78/5 | **7.33/10** |\n| Summarization | 4.37/5 | 3.33/5 | 6.92/10 |\n| Conversation | 1.90/5 | 2.62/5 | 4.07/10 |\n\n### Key Observations\n\n- **Verifiable domains achieve ~94% accuracy**: reward signal is clear and learnable\n- **Summarization performs best in unverifiable**: structured task aligns with reasoning format\n- **Conversation underperforms**: model sometimes requests clarification instead of answering\n- **Common failures**: truncation (token limit) or missing closing tags","metadata":{}}]}