{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tunix hackathon notebook submission template\n\nThis template notebook is part of the [Google Tunix hackathon](https://www.kaggle.com/competitions/google-tunix-hackathon/overview) and is used to simplify the submission process for competition participants and streamline the evaluation process for the judges.","metadata":{"id":"abdhOBYHqYz6"}},{"cell_type":"markdown","source":"## Your overall training and evaluation strategy\n\nPlease discuss your overall thinking process on how to approach this hackathon, how you allocate your compute, how you set up the reward for RL, how you evaluate the model, what kind of techniques are used, how your training pipeline looks like, ablation studies you have done, etc. \n\nThink of this section as a mini-technical report.","metadata":{}},{"cell_type":"markdown","source":"## How your finetuning dataset is created\n\nConstructing a good data mixture is a key challenge for this hackathon. Please discuss this topic in detail in this section (but there is no need to put relevant code here since the judges will not try to reproduce your dataset).\n\nAlso make sure the data set is publicly accessible for evaluation.","metadata":{}},{"cell_type":"markdown","source":"## Your Tunix finetuning code\n\nUse instruction-tuned Gemma2 2B or Gemma3 1B (other models are not allowed).","metadata":{"id":"afofSj37qYz6"}},{"cell_type":"code","source":"# Your prompt\nPROMPT_TEMPLATE = \"your awesome prompt with a placeholder {question}\"\n\n# Training parameters; feel free to change\nTEMPERATURE=0.7\nTOP_K=50\nTOP_P=0.9\n\n# You may also change this parameter for inference if you want\nMAX_GENERATION_STEPS=768\n\n\n# DO NOT CHANGE BELOW\n\n# Use these standard output tags so that your model's output follow this format in plain text (no JSON/XML):\n# <reasoning>model_reasoning_trace</reasoning>\n# <answer>model_final_answer</answer>\n\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nSOLUTION_START = \"<answer>\"\nSOLUTION_END = \"</answer>\"\n\n# Use these parameters for greedy decoding; used in competition evaluation\nINF_TEMPERATURE=0\nINF_TOP_K=1\nINF_TOP_P=None\nSEED=42","metadata":{"id":"Z03GnyApTn1j","outputId":"ccd485a1-84b1-4d9f-bf45-4bf4a4c8ee7b","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T01:55:51.639279Z","iopub.execute_input":"2025-11-24T01:55:51.639428Z","iopub.status.idle":"2025-11-24T01:56:28.882786Z","shell.execute_reply.started":"2025-11-24T01:55:51.639414Z","shell.execute_reply":"2025-11-24T01:56:28.881652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here is your finetuning code:","metadata":{}},{"cell_type":"code","source":"# Your awesome finetuning code. Example:\n# \n# with mesh:\n#   grpo_trainer.train(train_dataset)\n#\n# Make sure at least one checkpoint is saved during a 9hr run.\n# The very last checkpoint will be used for evaluation.","metadata":{"id":"yJo2nuKB-wlw","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T01:58:10.183523Z","iopub.execute_input":"2025-11-24T01:58:10.183684Z","iopub.status.idle":"2025-11-24T01:58:10.191848Z","shell.execute_reply.started":"2025-11-24T01:58:10.183666Z","shell.execute_reply":"2025-11-24T01:58:10.191065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## [Optional 15pts] unrestricted mode\n\nIf you would like to participate in the unrestricted mode, please write down the Kaggle model ID for evaluation. Make sure that:\n1. you have published the relevant files (in Flax format) to Kaggle (please check Kaggle doc on how to upload model files. One way is to use `kagglehub.model_upload()`)\n2. you have set the model visibility to 'Public'\n3. the model files are loadable by Tunix modelling code (see the section below)\n4. the model uses the same base model as in your single-session run (e.g., if you use Gemma2 2B for single-session, then this mode needs to come from the same Gemma2 2B, NOT Gemma3 1B).","metadata":{"id":"s1NMAxMh0H5D"}},{"cell_type":"code","source":"# Example: 'windmaple/gpt2' in https://www.kaggle.com/models/windmaple/gpt2\n\nunrestricted_kaggle_model = \"user_name/model_name\"  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Other things you want the judges to know\n\nAdditional topics worth discussing. For example, \n- what you learned in this hackathon\n- challenges you faced during this hackathon (e.g., compute, Tunix product issues and feature requests)\n- how you think this hackathon could be better\n- if you made a PR to Tunix for this hackathon, make sure to include it here","metadata":{}},{"cell_type":"markdown","source":"# Competition evaluation\n\nThis section is for references only and do not need to be included in your submission.\n\nGoogle judges will first run your code above to reproduce your model and then run the code below for evaluation; the eval runtime is beyond your 9-hour budget so that you don't need to worry about it.\n\n## Load the final checkpoint for single-session mode evaluation\nThere is no need to manually merge the LoRA adapter; Tunix will take care of it at inference time.","metadata":{"id":"FzIP8glkTn1l"}},{"cell_type":"code","source":"# Define the checkpoint folder\nCKPT_DIR = 'your ckpt folder'\n\nimport re\n\n# Find the latest checkpoint by listing directories in CKPT_DIR/actor\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n  for item in os.listdir(actor_ckpt_dir):\n    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n      step = int(item)\n      if step > latest_step:\n        latest_step = step\n\nif latest_step == -1:\n  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval')  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)","metadata":{"id":"V-73HfP1Tn1l","trusted":true,"execution":{"execution_failed":"2025-11-24T03:13:32.807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create the sampler for finetuned model","metadata":{}},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + MAX_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"1vY9kl-ITn1l","trusted":true,"execution":{"execution_failed":"2025-11-24T03:13:32.807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\nIllustrative code for AI-based evaluation.","metadata":{}},{"cell_type":"code","source":"class TunixHackathonJudge:\n    questions = ['question1', 'question2', ...]\n    judge = \"ai\"\n    \n    def __init__(self, temperature, top_k, top_p, max_generation_steps, seed):\n        ...\n\n    def evaluate(self, sampler, prompt):\n        ...\n\nResult = TunixHackathonJudge(INF_TEMPERATURE, INF_TOP_K, INF_TOP_P, MAX_GENERATION_STEPS, SEED).evaluate(sampler, PROMPT_TEMPLATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unrestricted mode\n\nIf the participant includes a `unrestricted_kaggle_model`, Google will load the uploaded checkpoint from Kaggle like below and use the `TunixHackathonJudge` above for another eval.","metadata":{}},{"cell_type":"code","source":"trained_ckpt_path = kagglehub.model_download(unrestricted_kaggle_model+\"/jax/size\") # may need to append \"actor\"+str(latest_step)+\"model_params\" as well\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)\n\n# Evaluation code is pretty much the same as single-session mode above","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}